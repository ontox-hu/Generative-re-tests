{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288e6f9e-dd7d-4355-b495-adcb95941e80",
   "metadata": {},
   "source": [
    "# Evaluating the model after fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4228f4b-09c6-42c8-a705-e25226ab98c2",
   "metadata": {},
   "source": [
    "Use this notebook to get a quick insight into how the model performs. It shows the output of the model before training and after training for a few random exemples out of the valadation dataset. And runs an evaluation on the entire valadation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f40358-e630-4c2e-a1e2-eb517432b996",
   "metadata": {},
   "source": [
    "## Importing needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30446cb9-44dc-4b1f-8afd-423f530500f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgrootde/Generative-re-tests/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    AutoTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from wasabi import msg\n",
    "import random\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from os.path import abspath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b36a1f-736c-4f00-9a9a-de0e44b7c052",
   "metadata": {},
   "source": [
    "## Setting home directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aea61d60-6ae8-4f91-889a-8329d5d1ea68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Home directory: /home/lgrootde/Generative-re-tests\u001b[0m\n",
      "\u001b[38;5;4mℹ Selected config:\n",
      "/home/lgrootde/Generative-re-tests/config/config_T5-3b_cdr.yaml\u001b[0m\n",
      "\u001b[38;5;4mℹ Selected model:\n",
      "/home/lgrootde/data/generative_re_model_storage_azure/47/checkpoints/checkpoint-100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "home_dir = Path(abspath(\"\")).parent\n",
    "config_path = home_dir.joinpath(\"config/config_T5-3b_cdr.yaml\")\n",
    "trained_model_path = home_dir.parent.joinpath(\"data/generative_re_model_storage_azure/47/checkpoints/checkpoint-100\")\n",
    "\n",
    "msg.info(f\"Home directory: {home_dir}\")\n",
    "msg.info(f\"Selected config: {config_path}\")\n",
    "msg.info(f\"Selected model: {trained_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473b11ad-6e32-4421-aad2-2c35a5fcb7af",
   "metadata": {},
   "source": [
    "## Load Config & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ee923f6-dca8-4afa-be9a-16bb513a3fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config\n",
    "with open(config_path) as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13ad9288-daf4-4801-bcbe-448e71ad114c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 501 examples [00:00, 1421.59 examples/s]\n",
      "Generating validation split: 501 examples [00:00, 49712.47 examples/s]\n",
      "Generating test split: 501 examples [00:00, 55272.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "        config['dataset_vars']['type'], \n",
    "        data_dir=home_dir.joinpath(config['dataset_vars']['dir']),\n",
    "        column_names=config['dataset_vars']['column_names']\n",
    "        )\n",
    "\n",
    "eval_dataset = dataset['validation'].select(range(1,501)) # remove first row that contains column names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce827ac-8cbe-46cb-829e-7de9e7a07756",
   "metadata": {},
   "source": [
    "## Get the random examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9abf031-eac1-4679-836a-1bc258b52e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather random examples from the evaluation dataset\n",
    "amount_examples_to_show = 5\n",
    "random_examples = []\n",
    "for i in range(amount_examples_to_show):\n",
    "    pick = random.randint(0, len(eval_dataset)-1)\n",
    "    random_examples.append({'Input':eval_dataset[pick]['input'],\n",
    "                            'Expected output':eval_dataset[pick]['relations']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635f2145-e51d-45ba-a261-07eb2a39c7e9",
   "metadata": {},
   "source": [
    "## Performance before traing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47975b07-a490-4c39-89a3-62d5dc9b198b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgrootde/Generative-re-tests/venv/lib/python3.12/site-packages/transformers/models/t5/tokenization_t5_fast.py:171: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on google-t5/t5-3b automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = config['model_name']\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "global tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, legacy=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map\n",
    ") # we specificly use T5 for Conditional generations because it has a language modeling head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58a7565d-81b7-4fd3-9fe6-2f10c6a72e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "5 flourouracil-induced apical ballooning syndrome: a case report. The apical ballooning syndrome (ABS) is a recently described stress-mediated acute cardiac syndrome characterized by transient wall-motion abnormalities involving the apex and midventricle with hyperkinesis of the basal left ventricular (LV) segments without obstructive epicardial coronary disease. Cardiotoxicity is not an uncommon adverse effect of chemotherapeutic agents. However, there are no reports of ABS secondary to chemotherapeutic agents. We describe the case of a woman who developed the syndrome after chemotherapy for metastatic cancer. A 79-year-old woman presented with typical ischemic chest pain, elevated cardiac enzymes with significant ST-segment abnormalities on her electrocardiogram. She underwent recent chemotherapy with fluorouracil for metastatic colorectal cancer. Echocardiography revealed a wall-motion abnormality involving the apical and periapical segments which appeared akinetic. Coronary angiography revealed no obstructive coronary lesions. The patient was stabilized with medical therapy. Four weeks later she remained completely asymptomatic. Echocardiogram revealed a normal ejection fraction and a resolution of the apical akinesis. Pathogenetic mechanisms of cardiac complications in cancer patients undergoing chemotherapy include coronary vasospasm, endothelial damage and consequent thrombus formation. In our patient, both supraphysiologic levels of plasma catecholamines and stress related neuropeptides caused by cancer diagnosis as well as chemotherapy may have contributed the development of ABS.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "5 flourouracil ; fluorouracil @CHEMICAL@ apical ballooning syndrome ; abs @DISEASE@ @CID@ 5 flourouracil ; fluorouracil @CHEMICAL@ chest pain @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "the apical ballooning syndrome: a case report. \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Transient hemiparesis: a rare manifestation of diphenylhydantoin toxicity. Report of two cases. Among the common side effects of diphenylhydantoin (DPH) overdose, the most frequently encountered neurological signs are those of cerebellar dysfunction. Very rarely, the toxic neurological manifestations of this drug are of cerebral origin. Two patients are presented who suffered progressive hemiparesis due to DPH overdose. Both had brain surgery before DPH treatment. It is assumed that patients with some cerebral damage are liable to manifest DPH toxicity as focal neurological signs.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "diphenylhydantoin ; dph @CHEMICAL@ hemiparesis @DISEASE@ @CID@ diphenylhydantoin ; dph @CHEMICAL@ overdose @DISEASE@ @CID@ diphenylhydantoin ; dph @CHEMICAL@ cerebellar dysfunction @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "...      Diphenylhydantoin toxicity: report of two cases. Report of two cases. \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Effects of calcitonin on rat extrapyramidal motor system: behavioral and biochemical data. The effects of i.v.c. injection of human and salmon calcitonin on biochemical and behavioral parameters related to the extrapyramidal motor system, were investigated in male rats. Calcitonin injection resulted in a potentiation of haloperidol-induced catalepsy and a partial prevention of apomorphine-induced hyperactivity. Moreover calcitonin induced a significant decrease in nigral GAD activity but no change in striatal DA and DOPAC concentration or GAD activity. The results are discussed in view of a primary action of calcitonin on the striatonigral GABAergic pathway mediating the DA-related behavioral messages of striatal origin.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "haloperidol @CHEMICAL@ catalepsy @DISEASE@ @CID@ apomorphine @CHEMICAL@ hyperactivity @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "................... The effects of i.v.c. injection of human and salmon calcitonin on rat extrapyramidal motor system: behavioral and biochemical data. were investigated in male rats.. \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "High fat diet-fed obese rats are highly sensitive to doxorubicin-induced cardiotoxicity. Often, chemotherapy by doxorubicin (Adriamycin) is limited due to life threatening cardiotoxicity in patients during and posttherapy. Recently, we have shown that moderate diet restriction remarkably protects against doxorubicin-induced cardiotoxicity. This cardioprotection is accompanied by decreased cardiac oxidative stress and triglycerides and increased cardiac fatty-acid oxidation, ATP synthesis, and upregulated JAK/STAT3 pathway. In the current study, we investigated whether a physiological intervention by feeding 40% high fat diet (HFD), which induces obesity in male Sprague-Dawley rats (250-275 g), sensitizes to doxorubicin-induced cardiotoxicity. A LD(10) dose (8 mg doxorubicin/kg, ip) administered on day 43 of the HFD feeding regimen led to higher cardiotoxicity, cardiac dysfunction, lipid peroxidation, and 80% mortality in the obese (OB) rats in the absence of any significant renal or hepatic toxicity. Doxorubicin toxicokinetics studies revealed no change in accumulation of doxorubicin and doxorubicinol (toxic metabolite) in the normal diet-fed (ND) and OB hearts. Mechanistic studies revealed that OB rats are sensitized due to: (1) higher oxyradical stress leading to upregulation of uncoupling proteins 2 and 3, (2) downregulation of cardiac peroxisome proliferators activated receptor-alpha, (3) decreased plasma adiponectin levels, (4) decreased cardiac fatty-acid oxidation (666.9+/-14.0 nmol/min/g heart in ND versus 400.2+/-11.8 nmol/min/g heart in OB), (5) decreased mitochondrial AMP-alpha2 protein kinase, and (6) 86% drop in cardiac ATP levels accompanied by decreased ATP/ADP ratio after doxorubicin administration. Decreased cardiac erythropoietin and increased SOCS3 further downregulated the cardioprotective JAK/STAT3 pathway. In conclusion, HFD-induced obese rats are highly sensitized to doxorubicin-induced cardiotoxicity by substantially downregulating cardiac mitochondrial ATP generation, increasing oxidative stress and downregulating the JAK/STAT3 pathway.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "fat @CHEMICAL@ obese ; obesity ; ob @DISEASE@ @CID@ doxorubicin ; adriamycin @CHEMICAL@ cardiac dysfunction @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "ororor oror orororororor     oxidative stress and upregulation of uncoupling protein 2 and 3. Moreover, doxorubicin-induced cardiotoxicity was accompanied by increased cardiac oxidative stress and upregulation of uncoupling protein 3 and phosphorylated Akt and Akt-    , Often  \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Apparent cure of rheumatoid arthritis by bone marrow transplantation. We describe the induction of sustained remissions and possible cure of severe erosive rheumatoid arthritis (RA) by bone marrow transplantation (BMT) in 2 patients. BMT was used to treat severe aplastic anemia which was caused by gold in one case and D-penicillamine in the other. In the 8 and 6 years since the transplants (representing 8 and 4 years since cessation of all immunosuppressive therapy, respectively), the RA in each case has been completely quiescent. Although short term remission of severe RA following BMT has been reported, these are the first cases for which prolonged followup has been available. This experience raises the question of the role of BMT itself as a therapeutic option for patients with uncontrolled destructive synovitis.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "gold @CHEMICAL@ aplastic anemia @DISEASE@ @CID@ d-penicillamine @CHEMICAL@ aplastic anemia @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "................... Apparent cure of rheumatoid arthritis by bone marrow transplantation. BMT is a therapeutic option for severe aplastic anemia. BMT is a therapeutic option for severe aplastic anemia. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model performance before training\n",
    "inputs = [i[\"Input\"] for i in random_examples]\n",
    "expected_output = [i[\"Expected output\"] for i in random_examples]\n",
    "\n",
    "for input, expected in zip(inputs, expected_output):\n",
    "    # inference\n",
    "    input_ids = tokenizer(input, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "    output = model.generate(input_ids, max_new_tokens=128)\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # print overview\n",
    "    msg.info(\"Input:\")\n",
    "    print(input)\n",
    "    msg.good(\"Expected output:\")\n",
    "    print(expected)\n",
    "    msg.info(\"Actual output:\")\n",
    "    print(decoded_output, \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f04cea3-be66-4b5c-937f-8e034015bae7",
   "metadata": {},
   "source": [
    "## Performance after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c54f4949-eec2-4e83-9312-f09db0d9b084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [04:49<00:00, 96.36s/it] \n"
     ]
    }
   ],
   "source": [
    "# Load model after training\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    trained_model_path,\n",
    "    device_map=device_map,\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65dfebe5-1ed1-42ab-9b27-093edbdb5877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "5 flourouracil-induced apical ballooning syndrome: a case report. The apical ballooning syndrome (ABS) is a recently described stress-mediated acute cardiac syndrome characterized by transient wall-motion abnormalities involving the apex and midventricle with hyperkinesis of the basal left ventricular (LV) segments without obstructive epicardial coronary disease. Cardiotoxicity is not an uncommon adverse effect of chemotherapeutic agents. However, there are no reports of ABS secondary to chemotherapeutic agents. We describe the case of a woman who developed the syndrome after chemotherapy for metastatic cancer. A 79-year-old woman presented with typical ischemic chest pain, elevated cardiac enzymes with significant ST-segment abnormalities on her electrocardiogram. She underwent recent chemotherapy with fluorouracil for metastatic colorectal cancer. Echocardiography revealed a wall-motion abnormality involving the apical and periapical segments which appeared akinetic. Coronary angiography revealed no obstructive coronary lesions. The patient was stabilized with medical therapy. Four weeks later she remained completely asymptomatic. Echocardiogram revealed a normal ejection fraction and a resolution of the apical akinesis. Pathogenetic mechanisms of cardiac complications in cancer patients undergoing chemotherapy include coronary vasospasm, endothelial damage and consequent thrombus formation. In our patient, both supraphysiologic levels of plasma catecholamines and stress related neuropeptides caused by cancer diagnosis as well as chemotherapy may have contributed the development of ABS.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "5 flourouracil ; fluorouracil @CHEMICAL@ apical ballooning syndrome ; abs @DISEASE@ @CID@ 5 flourouracil ; fluorouracil @CHEMICAL@ chest pain @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "flourouracil @CHEMICAL@ apical ballooning syndrome ; abs @DISEASE@ @CID@ \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Transient hemiparesis: a rare manifestation of diphenylhydantoin toxicity. Report of two cases. Among the common side effects of diphenylhydantoin (DPH) overdose, the most frequently encountered neurological signs are those of cerebellar dysfunction. Very rarely, the toxic neurological manifestations of this drug are of cerebral origin. Two patients are presented who suffered progressive hemiparesis due to DPH overdose. Both had brain surgery before DPH treatment. It is assumed that patients with some cerebral damage are liable to manifest DPH toxicity as focal neurological signs.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "diphenylhydantoin ; dph @CHEMICAL@ hemiparesis @DISEASE@ @CID@ diphenylhydantoin ; dph @CHEMICAL@ overdose @DISEASE@ @CID@ diphenylhydantoin ; dph @CHEMICAL@ cerebellar dysfunction @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "diphenylhydantoin ; dph @CHEMICAL@ hemiparesis @DISEASE@ @CID@ \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Effects of calcitonin on rat extrapyramidal motor system: behavioral and biochemical data. The effects of i.v.c. injection of human and salmon calcitonin on biochemical and behavioral parameters related to the extrapyramidal motor system, were investigated in male rats. Calcitonin injection resulted in a potentiation of haloperidol-induced catalepsy and a partial prevention of apomorphine-induced hyperactivity. Moreover calcitonin induced a significant decrease in nigral GAD activity but no change in striatal DA and DOPAC concentration or GAD activity. The results are discussed in view of a primary action of calcitonin on the striatonigral GABAergic pathway mediating the DA-related behavioral messages of striatal origin.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "haloperidol @CHEMICAL@ catalepsy @DISEASE@ @CID@ apomorphine @CHEMICAL@ hyperactivity @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "haloperidol @CHEMICAL@ catalepsy @DISEASE@ @CID@ apomorphine @CHEMICAL@ hyperactivity @DISEASE@ @CID@ \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "High fat diet-fed obese rats are highly sensitive to doxorubicin-induced cardiotoxicity. Often, chemotherapy by doxorubicin (Adriamycin) is limited due to life threatening cardiotoxicity in patients during and posttherapy. Recently, we have shown that moderate diet restriction remarkably protects against doxorubicin-induced cardiotoxicity. This cardioprotection is accompanied by decreased cardiac oxidative stress and triglycerides and increased cardiac fatty-acid oxidation, ATP synthesis, and upregulated JAK/STAT3 pathway. In the current study, we investigated whether a physiological intervention by feeding 40% high fat diet (HFD), which induces obesity in male Sprague-Dawley rats (250-275 g), sensitizes to doxorubicin-induced cardiotoxicity. A LD(10) dose (8 mg doxorubicin/kg, ip) administered on day 43 of the HFD feeding regimen led to higher cardiotoxicity, cardiac dysfunction, lipid peroxidation, and 80% mortality in the obese (OB) rats in the absence of any significant renal or hepatic toxicity. Doxorubicin toxicokinetics studies revealed no change in accumulation of doxorubicin and doxorubicinol (toxic metabolite) in the normal diet-fed (ND) and OB hearts. Mechanistic studies revealed that OB rats are sensitized due to: (1) higher oxyradical stress leading to upregulation of uncoupling proteins 2 and 3, (2) downregulation of cardiac peroxisome proliferators activated receptor-alpha, (3) decreased plasma adiponectin levels, (4) decreased cardiac fatty-acid oxidation (666.9+/-14.0 nmol/min/g heart in ND versus 400.2+/-11.8 nmol/min/g heart in OB), (5) decreased mitochondrial AMP-alpha2 protein kinase, and (6) 86% drop in cardiac ATP levels accompanied by decreased ATP/ADP ratio after doxorubicin administration. Decreased cardiac erythropoietin and increased SOCS3 further downregulated the cardioprotective JAK/STAT3 pathway. In conclusion, HFD-induced obese rats are highly sensitized to doxorubicin-induced cardiotoxicity by substantially downregulating cardiac mitochondrial ATP generation, increasing oxidative stress and downregulating the JAK/STAT3 pathway.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "fat @CHEMICAL@ obese ; obesity ; ob @DISEASE@ @CID@ doxorubicin ; adriamycin @CHEMICAL@ cardiac dysfunction @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "doxorubicin ; adriamycin @CHEMICAL@ cardiotoxicity @DISEASE@ @CID@ doxorubicin ; adriamycin @CHEMICAL@ cardiac dysfunction @DISEASE@ @CID@ doxorubicin ; adriamycin @CHEMICAL@ lipid peroxidation @DISEASE@ @CID@ \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Apparent cure of rheumatoid arthritis by bone marrow transplantation. We describe the induction of sustained remissions and possible cure of severe erosive rheumatoid arthritis (RA) by bone marrow transplantation (BMT) in 2 patients. BMT was used to treat severe aplastic anemia which was caused by gold in one case and D-penicillamine in the other. In the 8 and 6 years since the transplants (representing 8 and 4 years since cessation of all immunosuppressive therapy, respectively), the RA in each case has been completely quiescent. Although short term remission of severe RA following BMT has been reported, these are the first cases for which prolonged followup has been available. This experience raises the question of the role of BMT itself as a therapeutic option for patients with uncontrolled destructive synovitis.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "gold @CHEMICAL@ aplastic anemia @DISEASE@ @CID@ d-penicillamine @CHEMICAL@ aplastic anemia @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "d-penicillamine @CHEMICAL@ rheumatoid arthritis ; ra @DISEASE@ @CID@ \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model performance before training\n",
    "inputs = [i[\"Input\"] for i in random_examples]\n",
    "expected_output = [i[\"Expected output\"] for i in random_examples]\n",
    "\n",
    "for input, expected in zip(inputs, expected_output):\n",
    "    # inference\n",
    "    input_ids = tokenizer(input, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "    output = model.generate(input_ids, max_new_tokens=128)\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # print overview\n",
    "    msg.info(\"Input:\")\n",
    "    print(input)\n",
    "    msg.good(\"Expected output:\")\n",
    "    print(expected)\n",
    "    msg.info(\"Actual output:\")\n",
    "    print(decoded_output, \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff52e9-b631-4aa0-b192-3b052bd13b88",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Evaluation using scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42e1d59a-4caa-44ef-a799-cd1a17244c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, home_dir.__str__()) # Adds the home directory to the system path so that run.py can be imported\n",
    "from helper_functions import (\n",
    "    postprocess_text,\n",
    "    split_on_labels,\n",
    "    handle_coreforents,\n",
    "    extract_relation_triples,\n",
    "    get_group,\n",
    "    map_coferents,\n",
    "    split_coferents,\n",
    "    ner_metric,\n",
    "    re_metric,\n",
    ")\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54e042-d118-4dbb-898a-62a24235ff58",
   "metadata": {},
   "source": [
    "### Setting up trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29fd09e2-135c-4b8a-aa50-808615ea9866",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = Seq2SeqTrainingArguments(\n",
    "        output_dir=config['output_dir'],\n",
    "        per_device_train_batch_size=config['per_device_train_batch_size'],\n",
    "        gradient_accumulation_steps=config['gradient_accumulation_steps'],\n",
    "        optim=config['optim'],\n",
    "        save_steps=config['save_steps'],\n",
    "        logging_steps=config['logging_steps'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        fp16=config['fp16'],\n",
    "        bf16=config['bf16'],\n",
    "        max_grad_norm=config['max_grad_norm'],\n",
    "        max_steps=config['max_steps'],\n",
    "        warmup_ratio=config['warmup_ratio'],\n",
    "        group_by_length=config['group_by_length'],\n",
    "        lr_scheduler_type=config['lr_scheduler_type'],\n",
    "        predict_with_generate=True,\n",
    "        save_total_limit=2,\n",
    "        save_strategy='steps',\n",
    "        load_best_model_at_end=True,\n",
    "        do_eval=config['do_eval'],\n",
    "        evaluation_strategy=config['evaluation_strategy'],\n",
    "        eval_steps=config['eval_steps'],\n",
    "        remove_unused_columns=True,\n",
    "        generation_max_length=152\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4487f4f-7bb3-459a-b7f1-f741a732c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100,\n",
    "        pad_to_multiple_of=8 if config['fp16'] else None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0b2cee-43f0-40d0-8a96-2bb16deaa6d7",
   "metadata": {},
   "source": [
    "#### we implement an changed version of the preprocess function here because sacreds parameter injection is not availible here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d5bb2f9-b338-4835-97d9-33dacda80cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    '''\n",
    "    This function takes a dataset of input and target sequences.\n",
    "    meant to be used with the dataset.map() function\n",
    "    '''\n",
    "    \n",
    "    text_column = dataset_vars['column_names'][0]\n",
    "    rel_column = dataset_vars['column_names'][1]\n",
    "\n",
    "    # Split input and target\n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(examples[text_column])):\n",
    "        if examples[text_column][i] and examples[rel_column][i]: # remove pairs where one is None\n",
    "            inputs.append(examples[text_column][i])\n",
    "            targets.append(examples[rel_column][i])\n",
    "\n",
    "    # Tokenize the input\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=max_seq_length, \n",
    "        padding=padding, \n",
    "        truncation=truncation, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Tokenize the target sequence\n",
    "    labels = tokenizer(\n",
    "        text_target=targets, \n",
    "        max_length=max_seq_length, \n",
    "        padding=padding, \n",
    "        truncation=truncation,  \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Replace pad tokens with -100 so they don't contribute too the loss\n",
    "    if ignore_pad_token_for_loss:\n",
    "        labels[\"input_ids\"] = [\n",
    "                    [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "                ]\n",
    "\n",
    "    # Add tokenized target text to output\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71100b91-2fed-437b-9375-c02ca9ed8aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on train dataset: 100%|██████████| 500/500 [00:01<00:00, 265.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_vars = config['dataset_vars']\n",
    "max_seq_length = config['max_seq_length']\n",
    "padding = config['padding']\n",
    "truncation = config['truncation']\n",
    "ignore_pad_token_for_loss = config['ignore_pad_token_for_loss']\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Running tokenizer on train dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b931153d-5d35-4ef8-bb83-d3057db2ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "        \n",
    "    # Replace -100s used for padding as we can't decode them\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, rouge_types=['rouge1', 'rouge2'], references=decoded_labels, use_stemmer=False)\n",
    "    result.update(re_metric(predictions=decoded_preds, references=decoded_labels, ner_labels=['@CHEMICAL@', '@DISEASE@'], re_labels=['@CID@']))\n",
    "    result.update(ner_metric(predictions=decoded_preds, references=decoded_labels, ner_labels=['@CHEMICAL@', '@DISEASE@'], re_labels=['@CID@']))\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()} # rounds all metric values to 4 numvers behind the comma and make them percentages\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens) # mean length of the generated sequences\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0dc72c3-5be3-464b-9522-75154bbe694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metric\n",
    "global metric # Otherwise the metric object won't be accessible from within compute_metric()\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "737025b2-4b7e-4608-94cb-3e35cd1afc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        args=training_arguments,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96a06cb4-62d9-41d4-a7a6-7a629f6acdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 03:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Generative-re-tests/venv/lib/python3.12/site-packages/transformers/trainer_seq2seq.py:166\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Generative-re-tests/venv/lib/python3.12/site-packages/transformers/trainer.py:3229\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3226\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3228\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3229\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3230\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3233\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3239\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/Generative-re-tests/venv/lib/python3.12/site-packages/transformers/trainer.py:3520\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3516\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3517\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[1;32m   3518\u001b[0m         )\n\u001b[1;32m   3519\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3520\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3521\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3522\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_preds)\u001b[0m\n\u001b[1;32m     13\u001b[0m decoded_preds, decoded_labels \u001b[38;5;241m=\u001b[39m postprocess_text(decoded_preds, decoded_labels)\n\u001b[1;32m     15\u001b[0m result \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mdecoded_preds, rouge_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge2\u001b[39m\u001b[38;5;124m'\u001b[39m], references\u001b[38;5;241m=\u001b[39mdecoded_labels, use_stemmer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m result\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mre_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoded_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoded_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mner_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m@CHEMICAL@\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m@DISEASE@\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mre_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m@CID@\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     17\u001b[0m result\u001b[38;5;241m.\u001b[39mupdate(ner_metric(predictions\u001b[38;5;241m=\u001b[39mdecoded_preds, references\u001b[38;5;241m=\u001b[39mdecoded_labels, ner_labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@CHEMICAL@\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@DISEASE@\u001b[39m\u001b[38;5;124m'\u001b[39m], re_labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@CID@\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     18\u001b[0m result \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mround\u001b[39m(v \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;66;03m# rounds all metric values to 4 numvers behind the comma and make them percentages\u001b[39;00m\n",
      "File \u001b[0;32m~/Generative-re-tests/notebooks/helper_functions.py:202\u001b[0m, in \u001b[0;36mre_metric\u001b[0;34m(predictions, references, ner_labels, re_labels)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pred_text, ref_text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, references):\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 202\u001b[0m         predicted_triples \u001b[38;5;241m=\u001b[39m \u001b[43mextract_relation_triples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mner_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mre_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m: \u001b[38;5;66;03m# Text is unstructured\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         unstructured_text_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Generative-re-tests/notebooks/helper_functions.py:60\u001b[0m, in \u001b[0;36mextract_relation_triples\u001b[0;34m(text, ner_labels, re_labels, keep_coreforents)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText is unstructured: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mText should have 2 times the ner_labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mner_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m then there are re_label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mre_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. currently: ner labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount_ner_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | re labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount_re_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m##### Extracting relation triples #####\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Split the input text into relation segments\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m relation_segments \u001b[38;5;241m=\u001b[39m \u001b[43msplit_on_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mre_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Remove the last empty segment if it exists\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m relation_segments[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip():\n",
      "File \u001b[0;32m~/Generative-re-tests/notebooks/helper_functions.py:13\u001b[0m, in \u001b[0;36msplit_on_labels\u001b[0;34m(input_text, labels)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_on_labels\u001b[39m(input_text, labels):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Escape labels to ensure special characters are treated as literals in regex\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     escaped_labels \u001b[38;5;241m=\u001b[39m [\u001b[43mre\u001b[49m\u001b[38;5;241m.\u001b[39mescape(label) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Join the labels into a regex pattern with alternation to match any of them\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(escaped_labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_re",
   "language": "python",
   "name": "gen_re"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
