{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288e6f9e-dd7d-4355-b495-adcb95941e80",
   "metadata": {},
   "source": [
    "# Evaluating the model after fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4228f4b-09c6-42c8-a705-e25226ab98c2",
   "metadata": {},
   "source": [
    "Use this notebook to get a quick insight into how the model performs. It shows the output of the model before training and after training for a few random exemples out of the valadation dataset. And runs an evaluation on the entire valadation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f40358-e630-4c2e-a1e2-eb517432b996",
   "metadata": {},
   "source": [
    "## Importing needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30446cb9-44dc-4b1f-8afd-423f530500f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgrootde/Generative-re-tests/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    AutoTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from wasabi import msg\n",
    "import random\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from os.path import abspath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b36a1f-736c-4f00-9a9a-de0e44b7c052",
   "metadata": {},
   "source": [
    "## Setting home directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea61d60-6ae8-4f91-889a-8329d5d1ea68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Home directory: /home/lgrootde/Generative-re-tests\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "home_dir = Path(abspath(\"\")).parent\n",
    "\n",
    "msg.info(f\"Home directory: {home_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473b11ad-6e32-4421-aad2-2c35a5fcb7af",
   "metadata": {},
   "source": [
    "## Load Config & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee923f6-dca8-4afa-be9a-16bb513a3fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config\n",
    "with open(home_dir.joinpath('config/config_T5-L_cdr.yaml')) as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13ad9288-daf4-4801-bcbe-448e71ad114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "        config['dataset_vars']['type'], \n",
    "        data_dir=home_dir.joinpath(config['dataset_vars']['dir']),\n",
    "        column_names=config['dataset_vars']['column_names']\n",
    "        )\n",
    "\n",
    "eval_dataset = dataset['validation'].select(range(1,501)) # remove first row that contains column names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce827ac-8cbe-46cb-829e-7de9e7a07756",
   "metadata": {},
   "source": [
    "## Get the random examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9abf031-eac1-4679-836a-1bc258b52e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather random examples from the evaluation dataset\n",
    "amount_examples_to_show = 5\n",
    "random_examples = []\n",
    "for i in range(amount_examples_to_show):\n",
    "    pick = random.randint(0, len(eval_dataset)-1)\n",
    "    random_examples.append({'Input':eval_dataset[pick]['input'],\n",
    "                            'Expected output':eval_dataset[pick]['relations']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635f2145-e51d-45ba-a261-07eb2a39c7e9",
   "metadata": {},
   "source": [
    "## Performance before traing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47975b07-a490-4c39-89a3-62d5dc9b198b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgrootde/Generative-re-tests/venv/lib/python3.12/site-packages/transformers/models/t5/tokenization_t5_fast.py:171: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on google-t5/t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = config['model_name']\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "global tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, legacy=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map\n",
    ") # we specificly use T5 for Conditional generations because it has a language modeling head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58a7565d-81b7-4fd3-9fe6-2f10c6a72e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Cortical motor overactivation in parkinsonian patients with L-dopa-induced peak-dose dyskinesia. We have studied the regional cerebral blood flow (rCBF) changes induced by the execution of a finger-to-thumb opposition motor task in the supplementary and primary motor cortex of two groups of parkinsonian patients on L-dopa medication, the first one without L-dopa induced dyskinesia (n = 23) and the other with moderate peak-dose dyskinesia (n = 15), and of a group of 14 normal subjects. Single photon emission tomography with i.v. 133Xe was used to measure the rCBF changes. The dyskinetic parkinsonian patients exhibited a pattern of response which was markedly different from those of the normal subjects and non-dyskinetic parkinsonian patients, with a significant overactivation in the supplementary motor area and the ipsi- and contralateral primary motor areas. These results are compatible with the hypothesis that an hyperkinetic abnormal involuntary movement, like L-dopa-induced peak dose dyskinesia, is due to a disinhibition of the primary and associated motor cortex secondary to an excessive outflow of the pallidothalamocortical motor loop.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "l-dopa @CHEMICAL@ dyskinesia ; dyskinetic ; abnormal involuntary movement @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "-induced dyskinesia.... This is a symptom of a hyperkinetic abnormal involuntary movement, like L-dopa-induced peak-dose dyskinesia.. and and induced by.. n = 23.-induced  and and patients patients. n = 15..  motor area...... The supplementary \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Intraocular pressure in patients with uveitis treated with fluocinolone acetonide implants. OBJECTIVE: To report the incidence and management of elevated intraocular pressure (IOP) in patients with uveitis treated with the fluocinolone acetonide (FA) intravitreal implant. DESIGN: Pooled data from 3 multicenter, double-masked, randomized, controlled, phase 2b/3 clinical trials evaluating the safety and efficacy of the 0.59-mg or 2.1-mg FA intravitreal implant or standard therapy were analyzed. RESULTS: During the 3-year follow-up, 71.0% of implanted eyes had an IOP increase of 10 mm Hg or more than baseline and 55.1%, 24.7%, and 6.2% of eyes reached an IOP of 30 mm Hg or more, 40 mm Hg or more, and 50 mm Hg or more, respectively. Topical IOP-lowering medication was administered in 74.8% of implanted eyes, and IOP-lowering surgeries, most of which were trabeculectomies (76.2%), were performed on 36.6% of implanted eyes. Intraocular pressure-lowering surgeries were considered a success (postoperative IOP of 6-21 mm Hg with or without additional IOP-lowering medication) in 85.1% of eyes at 1 year. The rate of hypotony (IOP </= 5 mm Hg) following IOP-lowering surgery (42.5%) was not different from that of implanted eyes not subjected to surgery (35.4%) (P = .09). CONCLUSION: Elevated IOP is a significant complication with the FA intravitreal implant but may be controlled with medication and surgery.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "fluocinolone acetonide ; fa @CHEMICAL@ elevated intraocular pressure @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "/= 5 mm Hg) was not different from that of implanted eyes not subjected to surgery (P =.09). RESEARCH DESIGN: To report the incidence and management of elevated intraocular pressure in patients with uveitis treated with the FA intravitreal implant.. implant,, and or or standard or standard or standard or or or or or or or  \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Relation of perfusion defects observed with myocardial contrast echocardiography to the severity of coronary stenosis: correlation with thallium-201 single-photon emission tomography. It has been previously shown that myocardial contrast echocardiography is a valuable technique for delineating regions of myocardial underperfusion secondary to coronary occlusion and to critical coronary stenoses in the presence of hyperemic stimulation. The aim of this study was to determine whether myocardial contrast echocardiography performed with a stable solution of sonicated albumin could detect regions of myocardial underperfusion resulting from various degrees of coronary stenosis. The perfusion defect produced in 16 open chest dogs was compared with the anatomic area at risk measured by the postmortem dual-perfusion technique and with thallium-201 single-photon emission tomography (SPECT). During a transient (20-s) coronary occlusion, a perfusion defect was observed with contrast echocardiography in 14 of the 15 dogs in which the occlusion was produced. The perfusion defect correlated significantly with the anatomic area at risk (r = 0.74; p less than 0.002). During dipyridamole-induced hyperemia, 12 of the 16 dogs with a partial coronary stenosis had a visible area of hypoperfusion by contrast echocardiography. The four dogs without a perfusion defect had a stenosis that resulted in a mild (0% to 50%) reduction in dipyridamole-induced hyperemia. The size of the perfusion defect during stenosis correlated significantly with the anatomic area at risk (r = 0.61; p = 0.02). Thallium-201 SPECT demonstrated a perfusion defect in all 14 dogs analyzed during dipyridamole-induced hyperemia; the size of the perfusion defect correlated with the anatomic area at risk (r = 0.58; p less than 0.03) and with the perfusion defect by contrast echocardiography (r = 0.58; p less than 0.03). Thus, myocardial contrast echocardiography can be used to visualize and quantitate the amount of jeopardized myocardium during moderate to severe degrees of coronary stenosis. The results obtained show a correlation with the anatomic area at risk similar to that obtained with thallium-201 SPECT.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "dipyridamole @CHEMICAL@ hyperemic ; hyperemia @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "the size of the perfusion defect by contrast echocardiography and with the anatomic area at risk. The results obtained are similar to those obtained with thallium-201 SPECT. Fig. 1. Perfusion defect by myocardial contrast echocardiography and anatomic area at risk. Fig. 2. Perfusion defect by contrast echocardiography and SPECT \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Posteroventral medial pallidotomy in advanced Parkinson's disease. BACKGROUND: Posteroventral medial pallidotomy sometimes produces striking improvement in patients with advanced Parkinson's disease, but the studies to date have involved small numbers of patients and short-term follow-up. METHODS: Forty patients with Parkinson's disease underwent serial, detailed assessments both after drug withdrawal (\"off\" period) and while taking their optimal medical regimens (\"on\" period). All patients were examined preoperatively and 39 were examined at six months; 27 of the patients were also examined at one year, and 11 at two years. RESULTS: The percent improvements at six months were as follows: off-period score for overall motor function, 28 percent (95 percent confidence interval, 19 to 38 percent), with most of the improvement in the contralateral limbs; off-period score for activities of daily living, 29 percent (95 percent confidence interval, 19 to 39 percent); on-period score for contralateral dyskinesias, 82 percent (95 percent confidence interval, 72 to 91 percent); and on-period score for ipsilateral dyskinesias, 44 percent (95 percent confidence interval, 29 to 59 percent). The improvements in dyskinesias and the total scores for off-period parkinsonism, contralateral bradykinesia, and rigidity were sustained in the 11 patients examined at two years. The improvement in ipsilateral dyskinesias was lost after one year, and the improvements in postural stability and gait lasted only three to six months. Approximately half the patients who had been dependent on assistance in activities of daily living in the off period before surgery became independent after surgery. The complications of surgery were generally well tolerated, and there were no significant changes in the use of medication. CONCLUSIONS: In late-stage Parkinson's disease, pallidotomy significantly reduces levodopa-induced dyskinesias and off-period disability. Much of the benefit is sustained at two years, although some improvements, such as those on the ipsilateral side and in axial symptoms, wane within the first year. The on-period symptoms that are resistant to dopaminergic therapy do not respond to pallidotomy.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "levodopa @CHEMICAL@ dyskinesias @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      ".. BACKGROUND: Posteroventral medial pallidotomy may produce striking improvement in advanced Parkinson's disease........ in late-stage Parkinson's disease. at 19 to 39 percent; 27 at one year, 11 at two years., 27 at one year, 11 at two years at and and period period period period.. \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Contribution of the sympathetic nervous system to salt-sensitivity in lifetime captopril-treated spontaneously hypertensive rats. OBJECTIVE: To test the hypothesis that, in lifetime captopril-treated spontaneously hypertensive rats (SHR), the sympathetic nervous system contributes importantly to the hypertensive effect of dietary sodium chloride supplementation. METHODS: Male SHR (aged 6 weeks) that had been treated from conception onward with either captopril or vehicle remained on a basal sodium chloride diet or were fed a high sodium chloride diet. After 2 weeks, the rats were subjected to ganglionic blockade and 2 days later, an infusion of clonidine. RESULTS: Lifetime captopril treatment significantly lowered mean arterial pressure in both groups. Intravenous infusion of the ganglionic blocker hexamethonium resulted in a rapid decline in MAP that eliminated the dietary sodium chloride-induced increase in MAP in both groups. Infusion of the central nervous system alpha2-adrenergic receptor agonist clonidine also resulted in a greater reduction in MAP in both groups of SHR that were fed the high (compared with the basal) sodium chloride diet. CONCLUSIONS: In both lifetime captopril-treated and control SHR, the sympathetic nervous system contributes to the pressor effects of a high sodium chloride diet.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "dietary sodium chloride @CHEMICAL@ hypertensive ; increase in map @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "-treated SHR. Contribution of the sympathetic nervous system to salt-sensitivity in lifetime captopril-treated spontaneously hypertensive rats. and contributes to the hypertensive effects of a high sodium chloride diet. and contributes to the hypertensive effects of sodium chloride supplementation. Contribution of the sympathetic nervous system to salt-sensitivity in lifetime captopril-treated spontaneously hypertensive rats. RESEARCH:..,. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model performance before training\n",
    "inputs = [i[\"Input\"] for i in random_examples]\n",
    "expected_output = [i[\"Expected output\"] for i in random_examples]\n",
    "\n",
    "for input, expected in zip(inputs, expected_output):\n",
    "    # inference\n",
    "    input_ids = tokenizer(input, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "    output = model.generate(input_ids, max_new_tokens=128)\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # print overview\n",
    "    msg.info(\"Input:\")\n",
    "    print(input)\n",
    "    msg.good(\"Expected output:\")\n",
    "    print(expected)\n",
    "    msg.info(\"Actual output:\")\n",
    "    print(decoded_output, \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f04cea3-be66-4b5c-937f-8e034015bae7",
   "metadata": {},
   "source": [
    "## Performance after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c54f4949-eec2-4e83-9312-f09db0d9b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model after training\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    home_dir.joinpath(\"results/checkpoint-1200\"),\n",
    "    device_map=device_map,\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65dfebe5-1ed1-42ab-9b27-093edbdb5877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Cortical motor overactivation in parkinsonian patients with L-dopa-induced peak-dose dyskinesia. We have studied the regional cerebral blood flow (rCBF) changes induced by the execution of a finger-to-thumb opposition motor task in the supplementary and primary motor cortex of two groups of parkinsonian patients on L-dopa medication, the first one without L-dopa induced dyskinesia (n = 23) and the other with moderate peak-dose dyskinesia (n = 15), and of a group of 14 normal subjects. Single photon emission tomography with i.v. 133Xe was used to measure the rCBF changes. The dyskinetic parkinsonian patients exhibited a pattern of response which was markedly different from those of the normal subjects and non-dyskinetic parkinsonian patients, with a significant overactivation in the supplementary motor area and the ipsi- and contralateral primary motor areas. These results are compatible with the hypothesis that an hyperkinetic abnormal involuntary movement, like L-dopa-induced peak dose dyskinesia, is due to a disinhibition of the primary and associated motor cortex secondary to an excessive outflow of the pallidothalamocortical motor loop.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "l-dopa @CHEMICAL@ dyskinesia ; dyskinetic ; abnormal involuntary movement @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "l-dopa @CHEMICAL@ dyskinesia @DISEASE@ @CID@ \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Intraocular pressure in patients with uveitis treated with fluocinolone acetonide implants. OBJECTIVE: To report the incidence and management of elevated intraocular pressure (IOP) in patients with uveitis treated with the fluocinolone acetonide (FA) intravitreal implant. DESIGN: Pooled data from 3 multicenter, double-masked, randomized, controlled, phase 2b/3 clinical trials evaluating the safety and efficacy of the 0.59-mg or 2.1-mg FA intravitreal implant or standard therapy were analyzed. RESULTS: During the 3-year follow-up, 71.0% of implanted eyes had an IOP increase of 10 mm Hg or more than baseline and 55.1%, 24.7%, and 6.2% of eyes reached an IOP of 30 mm Hg or more, 40 mm Hg or more, and 50 mm Hg or more, respectively. Topical IOP-lowering medication was administered in 74.8% of implanted eyes, and IOP-lowering surgeries, most of which were trabeculectomies (76.2%), were performed on 36.6% of implanted eyes. Intraocular pressure-lowering surgeries were considered a success (postoperative IOP of 6-21 mm Hg with or without additional IOP-lowering medication) in 85.1% of eyes at 1 year. The rate of hypotony (IOP </= 5 mm Hg) following IOP-lowering surgery (42.5%) was not different from that of implanted eyes not subjected to surgery (35.4%) (P = .09). CONCLUSION: Elevated IOP is a significant complication with the FA intravitreal implant but may be controlled with medication and surgery.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "fluocinolone acetonide ; fa @CHEMICAL@ elevated intraocular pressure @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "fluocinolone acetonide ; fa @CHEMICAL@ intraocular pressure ; iop @DISEASE@ @CID@ fluocinolone acetonide ; fa @CHEMICAL@ hypotony @DISEASE@ @CID@ \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Relation of perfusion defects observed with myocardial contrast echocardiography to the severity of coronary stenosis: correlation with thallium-201 single-photon emission tomography. It has been previously shown that myocardial contrast echocardiography is a valuable technique for delineating regions of myocardial underperfusion secondary to coronary occlusion and to critical coronary stenoses in the presence of hyperemic stimulation. The aim of this study was to determine whether myocardial contrast echocardiography performed with a stable solution of sonicated albumin could detect regions of myocardial underperfusion resulting from various degrees of coronary stenosis. The perfusion defect produced in 16 open chest dogs was compared with the anatomic area at risk measured by the postmortem dual-perfusion technique and with thallium-201 single-photon emission tomography (SPECT). During a transient (20-s) coronary occlusion, a perfusion defect was observed with contrast echocardiography in 14 of the 15 dogs in which the occlusion was produced. The perfusion defect correlated significantly with the anatomic area at risk (r = 0.74; p less than 0.002). During dipyridamole-induced hyperemia, 12 of the 16 dogs with a partial coronary stenosis had a visible area of hypoperfusion by contrast echocardiography. The four dogs without a perfusion defect had a stenosis that resulted in a mild (0% to 50%) reduction in dipyridamole-induced hyperemia. The size of the perfusion defect during stenosis correlated significantly with the anatomic area at risk (r = 0.61; p = 0.02). Thallium-201 SPECT demonstrated a perfusion defect in all 14 dogs analyzed during dipyridamole-induced hyperemia; the size of the perfusion defect correlated with the anatomic area at risk (r = 0.58; p less than 0.03) and with the perfusion defect by contrast echocardiography (r = 0.58; p less than 0.03). Thus, myocardial contrast echocardiography can be used to visualize and quantitate the amount of jeopardized myocardium during moderate to severe degrees of coronary stenosis. The results obtained show a correlation with the anatomic area at risk similar to that obtained with thallium-201 SPECT.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "dipyridamole @CHEMICAL@ hyperemic ; hyperemia @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "contrast echocardiography ; SPECT @CHEMICAL@ myocardial underperfusion @DISEASE@ @CID@ \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Posteroventral medial pallidotomy in advanced Parkinson's disease. BACKGROUND: Posteroventral medial pallidotomy sometimes produces striking improvement in patients with advanced Parkinson's disease, but the studies to date have involved small numbers of patients and short-term follow-up. METHODS: Forty patients with Parkinson's disease underwent serial, detailed assessments both after drug withdrawal (\"off\" period) and while taking their optimal medical regimens (\"on\" period). All patients were examined preoperatively and 39 were examined at six months; 27 of the patients were also examined at one year, and 11 at two years. RESULTS: The percent improvements at six months were as follows: off-period score for overall motor function, 28 percent (95 percent confidence interval, 19 to 38 percent), with most of the improvement in the contralateral limbs; off-period score for activities of daily living, 29 percent (95 percent confidence interval, 19 to 39 percent); on-period score for contralateral dyskinesias, 82 percent (95 percent confidence interval, 72 to 91 percent); and on-period score for ipsilateral dyskinesias, 44 percent (95 percent confidence interval, 29 to 59 percent). The improvements in dyskinesias and the total scores for off-period parkinsonism, contralateral bradykinesia, and rigidity were sustained in the 11 patients examined at two years. The improvement in ipsilateral dyskinesias was lost after one year, and the improvements in postural stability and gait lasted only three to six months. Approximately half the patients who had been dependent on assistance in activities of daily living in the off period before surgery became independent after surgery. The complications of surgery were generally well tolerated, and there were no significant changes in the use of medication. CONCLUSIONS: In late-stage Parkinson's disease, pallidotomy significantly reduces levodopa-induced dyskinesias and off-period disability. Much of the benefit is sustained at two years, although some improvements, such as those on the ipsilateral side and in axial symptoms, wane within the first year. The on-period symptoms that are resistant to dopaminergic therapy do not respond to pallidotomy.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "levodopa @CHEMICAL@ dyskinesias @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "levodopa @CHEMICAL@ dyskinesias ; dyskinesia @DISEASE@ @CID@ levodopa @CHEMICAL@ disability ; dyskinesia @DISEASE@ @CID@ \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Contribution of the sympathetic nervous system to salt-sensitivity in lifetime captopril-treated spontaneously hypertensive rats. OBJECTIVE: To test the hypothesis that, in lifetime captopril-treated spontaneously hypertensive rats (SHR), the sympathetic nervous system contributes importantly to the hypertensive effect of dietary sodium chloride supplementation. METHODS: Male SHR (aged 6 weeks) that had been treated from conception onward with either captopril or vehicle remained on a basal sodium chloride diet or were fed a high sodium chloride diet. After 2 weeks, the rats were subjected to ganglionic blockade and 2 days later, an infusion of clonidine. RESULTS: Lifetime captopril treatment significantly lowered mean arterial pressure in both groups. Intravenous infusion of the ganglionic blocker hexamethonium resulted in a rapid decline in MAP that eliminated the dietary sodium chloride-induced increase in MAP in both groups. Infusion of the central nervous system alpha2-adrenergic receptor agonist clonidine also resulted in a greater reduction in MAP in both groups of SHR that were fed the high (compared with the basal) sodium chloride diet. CONCLUSIONS: In both lifetime captopril-treated and control SHR, the sympathetic nervous system contributes to the pressor effects of a high sodium chloride diet.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "dietary sodium chloride @CHEMICAL@ hypertensive ; increase in map @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "captopril @CHEMICAL@ salt @DISEASE@ @CID@ \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model performance before training\n",
    "inputs = [i[\"Input\"] for i in random_examples]\n",
    "expected_output = [i[\"Expected output\"] for i in random_examples]\n",
    "\n",
    "for input, expected in zip(inputs, expected_output):\n",
    "    # inference\n",
    "    input_ids = tokenizer(input, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "    output = model.generate(input_ids, max_new_tokens=128)\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # print overview\n",
    "    msg.info(\"Input:\")\n",
    "    print(input)\n",
    "    msg.good(\"Expected output:\")\n",
    "    print(expected)\n",
    "    msg.info(\"Actual output:\")\n",
    "    print(decoded_output, \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff52e9-b631-4aa0-b192-3b052bd13b88",
   "metadata": {},
   "source": [
    "## Evaluation using scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42e1d59a-4caa-44ef-a799-cd1a17244c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, home_dir.__str__()) # Adds the home directory to the system path so that run.py can be imported\n",
    "from helper_functions import (\n",
    "    postprocess_text,\n",
    "    split_on_labels,\n",
    "    handle_coreforents,\n",
    "    extract_relation_triples,\n",
    "    get_group,\n",
    "    map_coferents,\n",
    "    split_coferents,\n",
    "    ner_metric,\n",
    "    re_metric,\n",
    ")\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54e042-d118-4dbb-898a-62a24235ff58",
   "metadata": {},
   "source": [
    "### Setting up trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29fd09e2-135c-4b8a-aa50-808615ea9866",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = Seq2SeqTrainingArguments(\n",
    "        output_dir=config['output_dir'],\n",
    "        per_device_train_batch_size=config['per_device_train_batch_size'],\n",
    "        gradient_accumulation_steps=config['gradient_accumulation_steps'],\n",
    "        optim=config['optim'],\n",
    "        save_steps=config['save_steps'],\n",
    "        logging_steps=config['logging_steps'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        fp16=config['fp16'],\n",
    "        bf16=config['bf16'],\n",
    "        max_grad_norm=config['max_grad_norm'],\n",
    "        max_steps=config['max_steps'],\n",
    "        warmup_ratio=config['warmup_ratio'],\n",
    "        group_by_length=config['group_by_length'],\n",
    "        lr_scheduler_type=config['lr_scheduler_type'],\n",
    "        predict_with_generate=True,\n",
    "        save_total_limit=2,\n",
    "        save_strategy='steps',\n",
    "        load_best_model_at_end=True,\n",
    "        do_eval=config['do_eval'],\n",
    "        evaluation_strategy=config['evaluation_strategy'],\n",
    "        eval_steps=config['eval_steps'],\n",
    "        remove_unused_columns=True,\n",
    "        generation_max_length=152\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4487f4f-7bb3-459a-b7f1-f741a732c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100,\n",
    "        pad_to_multiple_of=8 if config['fp16'] else None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0b2cee-43f0-40d0-8a96-2bb16deaa6d7",
   "metadata": {},
   "source": [
    "#### we implement an changed version of the preprocess function here because sacreds parameter injection is not availible here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d5bb2f9-b338-4835-97d9-33dacda80cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    '''\n",
    "    This function takes a dataset of input and target sequences.\n",
    "    meant to be used with the dataset.map() function\n",
    "    '''\n",
    "    \n",
    "    text_column = dataset_vars['column_names'][0]\n",
    "    rel_column = dataset_vars['column_names'][1]\n",
    "\n",
    "    # Split input and target\n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(examples[text_column])):\n",
    "        if examples[text_column][i] and examples[rel_column][i]: # remove pairs where one is None\n",
    "            inputs.append(examples[text_column][i])\n",
    "            targets.append(examples[rel_column][i])\n",
    "\n",
    "    # Tokenize the input\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=max_seq_length, \n",
    "        padding=padding, \n",
    "        truncation=truncation, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Tokenize the target sequence\n",
    "    labels = tokenizer(\n",
    "        text_target=targets, \n",
    "        max_length=max_seq_length, \n",
    "        padding=padding, \n",
    "        truncation=truncation,  \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Replace pad tokens with -100 so they don't contribute too the loss\n",
    "    if ignore_pad_token_for_loss:\n",
    "        labels[\"input_ids\"] = [\n",
    "                    [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "                ]\n",
    "\n",
    "    # Add tokenized target text to output\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71100b91-2fed-437b-9375-c02ca9ed8aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on train dataset: 100%|██████████| 500/500 [00:01<00:00, 265.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_vars = config['dataset_vars']\n",
    "max_seq_length = config['max_seq_length']\n",
    "padding = config['padding']\n",
    "truncation = config['truncation']\n",
    "ignore_pad_token_for_loss = config['ignore_pad_token_for_loss']\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Running tokenizer on train dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b931153d-5d35-4ef8-bb83-d3057db2ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "        \n",
    "    # Replace -100s used for padding as we can't decode them\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, rouge_types=['rouge1', 'rouge2'], references=decoded_labels, use_stemmer=False)\n",
    "    result.update(re_metric(predictions=decoded_preds, references=decoded_labels, ner_labels=['@CHEMICAL@', '@DISEASE@'], re_labels=['@CID@']))\n",
    "    result.update(ner_metric(predictions=decoded_preds, references=decoded_labels, ner_labels=['@CHEMICAL@', '@DISEASE@'], re_labels=['@CID@']))\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()} # rounds all metric values to 4 numvers behind the comma and make them percentages\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens) # mean length of the generated sequences\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0dc72c3-5be3-464b-9522-75154bbe694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metric\n",
    "global metric # Otherwise the metric object won't be accessible from within compute_metric()\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "737025b2-4b7e-4608-94cb-3e35cd1afc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        args=training_arguments,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96a06cb4-62d9-41d4-a7a6-7a629f6acdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 03:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Generative-re-tests/venv/lib/python3.12/site-packages/transformers/trainer_seq2seq.py:166\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Generative-re-tests/venv/lib/python3.12/site-packages/transformers/trainer.py:3229\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3226\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3228\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3229\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3230\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3233\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3239\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/Generative-re-tests/venv/lib/python3.12/site-packages/transformers/trainer.py:3520\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3516\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3517\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[1;32m   3518\u001b[0m         )\n\u001b[1;32m   3519\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3520\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3521\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3522\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_preds)\u001b[0m\n\u001b[1;32m     13\u001b[0m decoded_preds, decoded_labels \u001b[38;5;241m=\u001b[39m postprocess_text(decoded_preds, decoded_labels)\n\u001b[1;32m     15\u001b[0m result \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mdecoded_preds, rouge_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge2\u001b[39m\u001b[38;5;124m'\u001b[39m], references\u001b[38;5;241m=\u001b[39mdecoded_labels, use_stemmer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m result\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mre_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoded_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoded_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mner_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m@CHEMICAL@\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m@DISEASE@\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mre_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m@CID@\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     17\u001b[0m result\u001b[38;5;241m.\u001b[39mupdate(ner_metric(predictions\u001b[38;5;241m=\u001b[39mdecoded_preds, references\u001b[38;5;241m=\u001b[39mdecoded_labels, ner_labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@CHEMICAL@\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@DISEASE@\u001b[39m\u001b[38;5;124m'\u001b[39m], re_labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@CID@\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     18\u001b[0m result \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mround\u001b[39m(v \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;66;03m# rounds all metric values to 4 numvers behind the comma and make them percentages\u001b[39;00m\n",
      "File \u001b[0;32m~/Generative-re-tests/notebooks/helper_functions.py:202\u001b[0m, in \u001b[0;36mre_metric\u001b[0;34m(predictions, references, ner_labels, re_labels)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pred_text, ref_text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, references):\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 202\u001b[0m         predicted_triples \u001b[38;5;241m=\u001b[39m \u001b[43mextract_relation_triples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mner_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mre_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m: \u001b[38;5;66;03m# Text is unstructured\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         unstructured_text_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Generative-re-tests/notebooks/helper_functions.py:60\u001b[0m, in \u001b[0;36mextract_relation_triples\u001b[0;34m(text, ner_labels, re_labels, keep_coreforents)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText is unstructured: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mText should have 2 times the ner_labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mner_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m then there are re_label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mre_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. currently: ner labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount_ner_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | re labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount_re_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m##### Extracting relation triples #####\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Split the input text into relation segments\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m relation_segments \u001b[38;5;241m=\u001b[39m \u001b[43msplit_on_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mre_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Remove the last empty segment if it exists\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m relation_segments[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip():\n",
      "File \u001b[0;32m~/Generative-re-tests/notebooks/helper_functions.py:13\u001b[0m, in \u001b[0;36msplit_on_labels\u001b[0;34m(input_text, labels)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_on_labels\u001b[39m(input_text, labels):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Escape labels to ensure special characters are treated as literals in regex\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     escaped_labels \u001b[38;5;241m=\u001b[39m [\u001b[43mre\u001b[49m\u001b[38;5;241m.\u001b[39mescape(label) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Join the labels into a regex pattern with alternation to match any of them\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(escaped_labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_re",
   "language": "python",
   "name": "gen_re"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
