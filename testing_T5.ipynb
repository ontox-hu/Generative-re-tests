{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a4e95d1-bf05-4f44-be56-ba5eb7d6aad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgrootde/Generative-re-tests/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/lgrootde/Generative-re-tests/venv/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/lgrootde/Generative-re-tests/venv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lgrootde/Generative-re-tests/venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from os.path import abspath\n",
    "from pathlib import Path\n",
    "from dataclasses import field\n",
    "from typing import Optional\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    AutoTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from wasabi import msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d38a629-3a77-4bbc-9719-693571022df9",
   "metadata": {},
   "source": [
    "# Load Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa8b51d-9571-4ff2-814a-3879fe891581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config\n",
    "import yaml\n",
    "with open('config/config_testing.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4189c844-5f2a-4b90-8b17-dd8928089e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"model_name\": \"google/t5-v1_1-base\",\n",
      "    \"dataset_vars\": {\n",
      "        \"type\": \"csv\",\n",
      "        \"dir\": \"data/cdr_seq2rel\",\n",
      "        \"split\": \"train\",\n",
      "        \"column_names\": [\n",
      "            \"input\",\n",
      "            \"relations\"\n",
      "        ]\n",
      "    },\n",
      "    \"output_dir\": \"./fine_tune_results\",\n",
      "    \"local_rank\": -1,\n",
      "    \"per_device_train_batch_size\": 1,\n",
      "    \"per_device_eval_batch_size\": 1,\n",
      "    \"gradient_accumulation_steps\": 8,\n",
      "    \"learning_rate\": 0.0002,\n",
      "    \"max_grad_norm\": 0.3,\n",
      "    \"weight_decay\": 0.001,\n",
      "    \"max_seq_length\": 512,\n",
      "    \"max_target_length\": 512,\n",
      "    \"num_train_epochs\": 0.5,\n",
      "    \"packing\": false,\n",
      "    \"padding\": true,\n",
      "    \"ignore_pad_token_for_loss\": true,\n",
      "    \"truncation\": true,\n",
      "    \"predict_with_generate\": true,\n",
      "    \"gradient_checkpointing\": true,\n",
      "    \"optim\": \"paged_adamw_32bit\",\n",
      "    \"lr_scheduler_type\": \"constant\",\n",
      "    \"max_steps\": 12,\n",
      "    \"warmup_ratio\": 0.03,\n",
      "    \"group_by_length\": true,\n",
      "    \"save_steps\": 4,\n",
      "    \"do_eval\": true,\n",
      "    \"evaluation_strategy\": \"steps\",\n",
      "    \"eval_steps\": 4,\n",
      "    \"pytorch_cuda_alloc_conf_list\": [\n",
      "        \"heuristic\",\n",
      "        \"max_split_size_mb512\"\n",
      "    ],\n",
      "    \"use_4bit\": false,\n",
      "    \"use_nested_quant\": false,\n",
      "    \"use_8bit\": false,\n",
      "    \"bnb_8bit\": {\n",
      "        \"compute_dtype\": \"float16\",\n",
      "        \"quant_type\": \"nf4\"\n",
      "    },\n",
      "    \"fp16\": false,\n",
      "    \"bf16\": false,\n",
      "    \"logging_steps\": 10\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Pretty print the config using json as a shortcut\n",
    "import json\n",
    "print(json.dumps(config, sort_keys=False, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3901495c-f095-43b8-9b97-b6b942c75f3d",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d144e56-fbc5-4d79-9df3-f51e1de3e7c4",
   "metadata": {},
   "source": [
    "First we'll load the dataset. The dataset used is the [CDR dataset in seq2rel format](https://github.com/JohnGiorgi/seq2rel-ds), This dataset is slightly altered to make it easier to use with [huggingface dataset](https://huggingface.co/docs/datasets/index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "185367d7-734b-4799-ac47-caeef0072892",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "        config['dataset_vars']['type'], \n",
    "        data_dir=config['dataset_vars']['dir'],\n",
    "        column_names=config['dataset_vars']['column_names']\n",
    "        )\n",
    "\n",
    "dataset_train = dataset['train'].select(range(1,501)) # remove first row that contains column names\n",
    "dataset_eval = dataset['validation'].select(range(1,501)) # remove first row that contains column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f22b262-e2f7-453c-a4eb-3b7091b3f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful function to see a random row in the dataset\n",
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6823529-b4f1-4473-9fa1-a0fb1306daf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>relations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Central nervous system complications during treatment of acute lymphoblastic leukemia in a single pediatric institution. Central nervous system (CNS) complications during treatment of childhood acute lymphoblastic leukemia (ALL) remain a challenging clinical problem. Outcome improvement with more intensive chemotherapy has significantly increased the incidence and severity of adverse events. This study analyzed the incidence of neurological complications during ALL treatment in a single pediatric institution, focusing on clinical, radiological, and electrophysiological findings. Exclusion criteria included CNS leukemic infiltration at diagnosis, therapy-related peripheral neuropathy, late-onset encephalopathy, or long-term neurocognitive defects. During a 9-year period, we retrospectively collected 27 neurological events (11%) in as many patients, from 253 children enrolled in the ALL front-line protocol. CNS complications included posterior reversible leukoencephalopathy syndrome (n = 10), stroke (n = 5), temporal lobe epilepsy (n = 2), high-dose methotrexate toxicity (n = 2), syndrome of inappropriate antidiuretic hormone secretion (n = 1), and other unclassified events (n = 7). In conclusion, CNS complications are frequent events during ALL therapy, and require rapid detection and prompt treatment to limit permanent damage.</td>\n",
       "      <td>methotrexate @CHEMICAL@ central nervous system complications ; central nervous system (cns) complications ; neurological complications ; neurocognitive defects @DISEASE@ @CID@</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset_train, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d843e66-6c4d-4b84-9acd-1e4e4eead779",
   "metadata": {},
   "source": [
    "The dataset has 2 columns, \"Input\" which are abstracts from various papers from pubmed and \"relations\" which are the relations between chemicals and diseases. The relations are in an novel format explained in the [seq2rel paper](https://aclanthology.org/2022.bionlp-1.2/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4367a75a-305f-4984-89ce-04d6d64c6dbc",
   "metadata": {},
   "source": [
    "# Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64b699fd-6ce8-4b6e-94ad-9ff916b7d982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not connected to a GPU\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8c3dbb8-88c0-422c-94e9-08c134087142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m device_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mT5ForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m \u001b[38;5;66;03m# we specificly use T5 for COnfitional generations because it has a language modeling head\u001b[39;00m\n",
      "File \u001b[0;32m~/Generative-re-tests/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:3170\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_pt:\n\u001b[1;32m   3168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded \u001b[38;5;129;01mand\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3169\u001b[0m         \u001b[38;5;66;03m# Time to load the checkpoint\u001b[39;00m\n\u001b[0;32m-> 3170\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3172\u001b[0m     \u001b[38;5;66;03m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[1;32m   3173\u001b[0m     \u001b[38;5;66;03m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[1;32m   3174\u001b[0m     \u001b[38;5;66;03m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[1;32m   3175\u001b[0m     \u001b[38;5;66;03m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[1;32m   3176\u001b[0m     \u001b[38;5;66;03m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[1;32m   3177\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Generative-re-tests/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:484\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m         map_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Generative-re-tests/venv/lib/python3.11/site-packages/torch/serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Generative-re-tests/venv/lib/python3.11/site-packages/torch/serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1427\u001b[0m )\n",
      "File \u001b[0;32m~/Generative-re-tests/venv/lib/python3.11/site-packages/torch/serialization.py:1392\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1391\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1392\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/Generative-re-tests/venv/lib/python3.11/site-packages/torch/serialization.py:1357\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset:storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1357\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_name = config['model_name']\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map\n",
    ") # we specificly use T5 for COnfitional generations because it has a language modeling head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df56776-6805-4a7c-8b77-01d9d1e25b6d",
   "metadata": {},
   "source": [
    "# Test tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ee0fa-ecf4-4071-8f1d-492b62e0a8bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show a single example of the dataset\n",
    "single_example = dataset_train[0]\n",
    "print(f\"A single row in the dataset is of type {type(single_example)}\")\n",
    "print(single_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b2a0c-4e47-4e69-8fd3-01b9f7882812",
   "metadata": {},
   "source": [
    "Before going into the model the data has to be tokenized. Tokenization is a preprocess where the text is split up into tokens, these tokens are then conferted to their respective token ids using the vocab, token ids can then be converted into word vectors which are the raw input of the model. The attention mask shows which tokens should be given attention to in the attention layer in the model. A 1 means the token is given attention to in the attention layer a 0 means the token is ignored in that process. This is important when using padding for example, you don't want the padding tokens to be factored in during the attention process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a512cc26-2719-46ba-be6d-6ebb02f571a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_input, column_name_output = config['dataset_vars']['column_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4008250-56c7-45eb-a0df-275499382552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show what the output of the tokenizer looks like for the input task\n",
    "tokenizer_output_text = tokenizer(single_example[column_name_input])\n",
    "tokenizer_output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e310c938-1e43-4f34-82bb-a795b6e7c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show what the output of the tokenizer looks like for the output text\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    tokenizer_output_rel = tokenizer(single_example[column_name_output])\n",
    "tokenizer_output_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99c20a8-9a3c-4482-9a58-a02df7f98086",
   "metadata": {},
   "source": [
    "Why tokenize the output? During training the model tasks is given a certain input match the expected output. To match it it needs to know where it's going wrong, so for the calculation of loss it needs the input ids of the tokenized output text. The exact training algorithm is presuambly [teacher forcing](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/). (I say presuamly here because i don't really know what training algoritm the huggingface api uses, where it's documentated, defined in the code and if it differs between models. And teacher forcing was used for training the T5 model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5ebd59-4e7c-4fa6-a089-e39c7ea39701",
   "metadata": {},
   "source": [
    "The T5 model has a max sequence input of 512. If an input exceed that it is truncated. This could cause the model to output the wrong relations because it is missing some information, certainly when a lot of important information output the conclusion of a paper is usally at the end of an abstract. So we'll check how many rows exceed the max sequence length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d87b52-da4a-4c07-8698-950fa9bc3f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(dataset_train[column_name_input])\n",
    "count = 0\n",
    "for idx, input_ids in enumerate(tokenized_dataset['input_ids']):\n",
    "    if len(input_ids) > 512:\n",
    "        # print(idx, len(input_ids))\n",
    "        count+=1\n",
    "print(f'Percentage that comes above max sequence length: {count/(idx+1)*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be7e66-dcfd-4dac-a6fd-48611e60a1c4",
   "metadata": {},
   "source": [
    "# Create Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a4504-9b5b-4f06-be6c-f6c8f8367fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_pad_token_for_loss=config['ignore_pad_token_for_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f3af2-710c-4b7d-b466-8c388313eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    '''\n",
    "    This function takes a dataset of input and target sequences.\n",
    "    meant to be used with the dataset.map() function\n",
    "    '''\n",
    "\n",
    "    column_name_input, column_name_output = config['dataset_vars']['column_names']\n",
    "\n",
    "    # Split input and target\n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(examples[column_name_input])):\n",
    "        if examples[column_name_input][i] and examples[column_name_output][i]: # remove pairs where one is None\n",
    "            inputs.append(examples[column_name_input][i])\n",
    "            targets.append(examples[column_name_output][i])\n",
    "\n",
    "    # Tokenize the input\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=config['max_seq_length'], \n",
    "        padding=config['padding'], \n",
    "        truncation=config['truncation'], \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Tokenize the target sequence\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets, \n",
    "            max_length=config['max_seq_length'], \n",
    "            padding=config['padding'], \n",
    "            truncation=config['truncation'], \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "    # Replace pad tokens with -100 so they don't contribute too the loss\n",
    "    if ignore_pad_token_for_loss:\n",
    "        labels[\"input_ids\"] = [\n",
    "                    [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "                ]\n",
    "\n",
    "    # Add tokenized target text to output\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25166871-1b9f-4225-a7ef-e9c1ed1a2261",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset_train.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Running tokenizer on train dataset\"\n",
    ")\n",
    "\n",
    "eval_dataset = dataset_eval.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Running tokenizer on train dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37e94f-0cc1-40eb-a6fb-6ecfa6767026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A look into the pre-tokenized dataset\n",
    "single_example = train_dataset[0]\n",
    "print(f\"A single row in the dataset is of type {type(single_example)}\")\n",
    "print(single_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7374863-ac54-48a7-a2aa-735d3605448b",
   "metadata": {},
   "source": [
    "# Create evaluation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4728ed6e-e6a8-4737-95ef-0e6be0afba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.generate(single_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5415e42d-0caf-40ca-83bb-7976c088d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b78e2-9a33-4b92-8e23-5cb96cd1c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3060b9e-de15-4fa3-9035-0a0c94fafa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_eval_example=[dataset_eval[0]['relations']]\n",
    "print(single_eval_example)\n",
    "# whats the output of metric.compute?\n",
    "metric.compute(\n",
    "    predictions=single_eval_example,\n",
    "    references=single_eval_example,\n",
    "    rouge_types=['rouge1', 'rouge2']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a7011-daaf-4806-8ffa-adde110c9927",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\"hello there\", \"general kenobi\"]\n",
    "references = [\"hello there\", \"general kenobi\"]\n",
    "metric.compute(predictions=predictions, references=references, rouge_types=['rouge1', 'rouge2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1fedda-d4e2-4422-b2cb-0e324d95478e",
   "metadata": {},
   "source": [
    "We need to define two functions:\n",
    "- a postprocess function that does the post processing of a prediction\n",
    "- A compute_metrics function that takes as input an prediction and output the mean score of that prediction?\n",
    "\n",
    "the compute metrics functions alwas takes a tuple of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32375a9b-d715-4656-9d85-c851a1d2e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "        preds = [pred.strip() for pred in preds]\n",
    "        labels = [label.strip() for label in labels]\n",
    "\n",
    "        # rougeLSum expects newline after each sentence\n",
    "        # preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "        # labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "        return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d403a11-daac-436e-92b9-3c64d6f92c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "        \n",
    "    # Replace -100s used for padding as we can't decode them\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=False)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()} # rounds all metric values to 4 numvers behind the comma\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens) # mean length of the generated sequences\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54e042-d118-4dbb-898a-62a24235ff58",
   "metadata": {},
   "source": [
    "# Setting up trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd09e2-135c-4b8a-aa50-808615ea9866",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = Seq2SeqTrainingArguments(\n",
    "        output_dir=config['output_dir'],\n",
    "        per_device_train_batch_size=config['per_device_train_batch_size'],\n",
    "        gradient_accumulation_steps=config['gradient_accumulation_steps'],\n",
    "        optim=config['optim'],\n",
    "        save_steps=config['save_steps'],\n",
    "        logging_steps=config['logging_steps'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        fp16=config['fp16'],\n",
    "        bf16=config['bf16'],\n",
    "        max_grad_norm=config['max_grad_norm'],\n",
    "        max_steps=config['max_steps'],\n",
    "        warmup_ratio=config['warmup_ratio'],\n",
    "        group_by_length=config['group_by_length'],\n",
    "        lr_scheduler_type=config['lr_scheduler_type'],\n",
    "        predict_with_generate=True,\n",
    "        save_total_limit=2,\n",
    "        save_strategy='steps',\n",
    "        load_best_model_at_end=True,\n",
    "        do_eval=config['do_eval'],\n",
    "        evaluation_strategy=config['evaluation_strategy'],\n",
    "        eval_steps=config['eval_steps']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4487f4f-7bb3-459a-b7f1-f741a732c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100,\n",
    "        pad_to_multiple_of=8 if config['fp16'] else None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737025b2-4b7e-4608-94cb-3e35cd1afc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        args=training_arguments,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6490531-fbb5-4f07-a16c-c075dd23a7ba",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d77396-81e3-4e57-80d0-28d004ddcb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465126b8-8241-40ab-9fcc-550148120e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_re",
   "language": "python",
   "name": "gen_re"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
