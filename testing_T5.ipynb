{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a4e95d1-bf05-4f44-be56-ba5eb7d6aad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgrootde/Generative-re-tests/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/lgrootde/Generative-re-tests/venv/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/lgrootde/Generative-re-tests/venv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lgrootde/Generative-re-tests/venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from os.path import abspath\n",
    "from pathlib import Path\n",
    "from dataclasses import field\n",
    "from typing import Optional\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    AutoTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from wasabi import msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3901495c-f095-43b8-9b97-b6b942c75f3d",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "185367d7-734b-4799-ac47-caeef0072892",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "        'csv', \n",
    "        data_dir=\"data/cdr_seq2rel\",\n",
    "        column_names=['input', 'relations']\n",
    "        )\n",
    "\n",
    "dataset_train = dataset['train'].select(range(1,501)) # remove first row that contains column names\n",
    "dataset_eval = dataset['validation'].select(range(1,501)) # remove first row that contains column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f22b262-e2f7-453c-a4eb-3b7091b3f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6823529-b4f1-4473-9fa1-a0fb1306daf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>relations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prenatal dexamethasone programs hypertension and renal injury in the rat. Dexamethasone is frequently administered to the developing fetus to accelerate pulmonary development. The purpose of the present study was to determine if prenatal dexamethasone programmed a progressive increase in blood pressure and renal injury in rats. Pregnant rats were given either vehicle or 2 daily intraperitoneal injections of dexamethasone (0.2 mg/kg body weight) on gestational days 11 and 12, 13 and 14, 15 and 16, 17 and 18, or 19 and 20. Offspring of rats administered dexamethasone on days 15 and 16 gestation had a 20% reduction in glomerular number compared with control at 6 to 9 months of age (22 527+/-509 versus 28 050+/-561, P&lt;0.05), which was comparable to the percent reduction in glomeruli measured at 3 weeks of age. Six- to 9-month old rats receiving prenatal dexamethasone on days 17 and 18 of gestation had a 17% reduction in glomeruli (23 380+/-587) compared with control rats (P&lt;0.05). Male rats that received prenatal dexamethasone on days 15 and 16, 17 and 18, and 13 and 14 of gestation had elevated blood pressures at 6 months of age; the latter group did not have a reduction in glomerular number. Adult rats given dexamethasone on days 15 and 16 of gestation had more glomeruli with glomerulosclerosis than control rats. This study shows that prenatal dexamethasone in rats results in a reduction in glomerular number, glomerulosclerosis, and hypertension when administered at specific points during gestation. Hypertension was observed in animals that had a reduction in glomeruli as well as in a group that did not have a reduction in glomerular number, suggesting that a reduction in glomerular number is not the sole cause for the development of hypertension.</td>\n",
       "      <td>dexamethasone @CHEMICAL@ hypertension ; increase in blood pressure ; elevated blood pressures @DISEASE@ @CID@ dexamethasone @CHEMICAL@ renal injury ; reduction in glomerular number @DISEASE@ @CID@</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lamivudine for the prevention of hepatitis B virus reactivation in hepatitis-B surface antigen (HBSAG) seropositive cancer patients undergoing cytotoxic chemotherapy. Hepatitis B virus (HBV) is one of the major causes of chronic liver disease worldwide. Cancer patients who are chronic carriers of HBV have a higher hepatic complication rate while receiving cytotoxic chemotherapy (CT) and this has mainly been attributed to HBV reactivation. In this study, cancer patients who have solid and hematological malignancies with chronic HBV infection received the antiviral agent lamivudine prior and during CT compared with historical control group who did not receive lamivudine. The objectives were to assess the efficacy of lamivudine in reducing the incidence of HBV reactivation, and diminishing morbidity and mortality during CT. Two groups were compared in this study. The prophylactic lamivudin group consisted of 37 patients who received prophylactic lamivudine treatment. The historical controls consisted of 50 consecutive patients who underwent CT without prophylactic lamivudine. They were followed up during and for 8 weeks after CT. The outcomes were compared for both groups. Of our control group (n= 50), 21 patients (42%) were established hepatitis. Twelve (24%) of them were evaluated as severe hepatitis. In the prophylactic lamivudine group severe hepatitis were observed only in 1 patient (2.7%) of 37 patients (p &lt; 0.006). Comparison of the mean ALT values revealed significantly higher mean alanine aminotransferase (ALT) values in the control group than the prophylactic lamivudine group; 154:64 (p &lt; 0.32). Our study suggests that prophylactic lamivudine significantly decreases the incidence of HBV reactivation and overall morbidity in cancer patients during and after immunosuppressive therapy. Further studies are needed to determine the most appropriate nucleoside or nucleotide analogue for antiviral prophylaxis during CT and the optimal duration of administration after completion of CT.</td>\n",
       "      <td>hepatitis-b surface antigen ; hbsag @CHEMICAL@ hepatitis b ; hbv infection @DISEASE@ @CID@</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acute confusion induced by a high-dose infusion of 5-fluorouracil and folinic acid. A 61-year-old man was treated with combination chemotherapy incorporating cisplatinum, etoposide, high-dose 5-fluorouracil (2,250 mg/m2/24 hours) and folinic acid for an inoperable gastric adenocarcinoma. He developed acute neurologic symptoms of mental confusion, disorientation and irritability, and then lapsed into a deep coma, lasting for approximately 40 hours during the first dose (day 2) of 5-fluorouracil and folinic acid infusion. This complication reappeared on day 25 during the second dose of 5-fluorouracil and folinic acid, which were then the only drugs given. Because folinic acid was unlikely to be associated with this condition, neurotoxicity due to high-dose 5-fluorouracil was highly suspected. The pathogenesis of 5-fluorouracil neurotoxicity may be due to a Krebs cycle blockade by fluoroacetate and fluorocitrate, thiamine deficiency, or dihydrouracil dehydrogenase deficiency. High-dose 5-fluorouracil/folinic acid infusion therapy has recently become a popular regimen for various cancers. It is necessary that both oncologists and neurologists be fully aware of this unusual complication.</td>\n",
       "      <td>5-fluorouracil @CHEMICAL@ confusion ; disorientation @DISEASE@ @CID@ 5-fluorouracil @CHEMICAL@ coma @DISEASE@ @CID@ folinic acid @CHEMICAL@ confusion ; disorientation @DISEASE@ @CID@ folinic acid @CHEMICAL@ coma @DISEASE@ @CID@</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acute cocaine-induced seizures: differential sensitivity of six inbred mouse strains. Mature male and female mice from six inbred stains were tested for susceptibility to behavioral seizures induced by a single injection of cocaine. Cocaine was injected ip over a range of doses (50-100 mg/kg) and behavior was monitored for 20 minutes. Seizure end points included latency to forelimb or hindlimb clonus, latency to clonic running seizure and latency to jumping bouncing seizure. A range of strain specific sensitivities was documented with A/J and SJL mice being most sensitive and C57BL/6J most resistant. DBA/2J, BALB/cByJ and NZW/LacJ strains exhibited intermediate sensitivity. EEG recordings were made in SJL, A/J and C57BL/6J mice revealing a close correspondence between electrical activity and behavior. Additionally, levels of cocaine determined in hippocampus and cortex were not different between sensitive and resistant strains. Additional studies of these murine strains may be useful for investigating genetic influences on cocaine-induced seizures.</td>\n",
       "      <td>cocaine @CHEMICAL@ seizures ; seizure @DISEASE@ @CID@</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Epithelial sodium channel (ENaC) subunit mRNA and protein expression in rats with puromycin aminonucleoside-induced nephrotic syndrome. In experimental nephrotic syndrome, urinary sodium excretion is decreased during the early phase of the disease. The molecular mechanism(s) leading to salt retention has not been completely elucidated. The rate-limiting constituent of collecting duct sodium transport is the epithelial sodium channel (ENaC). We examined the abundance of ENaC subunit mRNAs and proteins in puromycin aminonucleoside (PAN)-induced nephrotic syndrome. The time courses of urinary sodium excretion, plasma aldosterone concentration and proteinuria were studied in male Sprague-Dawley rats treated with a single dose of either PAN or vehicle. The relative amounts of alphaENaC, betaENaC and gammaENaC mRNAs were determined in kidneys from these rats by real-time quantitative TaqMan PCR, and the amounts of proteins by Western blot. The kinetics of urinary sodium excretion and the appearance of proteinuria were comparable with those reported previously. Sodium retention occurred on days 2, 3 and 6 after PAN injection. A significant up-regulation of alphaENaC and betaENaC mRNA abundance on days 1 and 2 preceded sodium retention on days 2 and 3. Conversely, down-regulation of alphaENaC, betaENaC and gammaENaC mRNA expression on day 3 occurred in the presence of high aldosterone concentrations, and was followed by a return of sodium excretion to control values. The amounts of alphaENaC, betaENaC and gammaENaC proteins were not increased during PAN-induced sodium retention. In conclusion, ENaC mRNA expression, especially alphaENaC, is increased in the very early phase of the experimental model of PAN-induced nephrotic syndrome in rats, but appears to escape from the regulation by aldosterone after day 3.</td>\n",
       "      <td>puromycin aminonucleoside ; pan @CHEMICAL@ nephrotic syndrome @DISEASE@ @CID@ puromycin aminonucleoside ; pan @CHEMICAL@ proteinuria @DISEASE@ @CID@</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4367a75a-305f-4984-89ce-04d6d64c6dbc",
   "metadata": {},
   "source": [
    "## Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8c3dbb8-88c0-422c-94e9-08c134087142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/t5-v1_1-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df56776-6805-4a7c-8b77-01d9d1e25b6d",
   "metadata": {},
   "source": [
    "## Test tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c26ee0fa-ecf4-4071-8f1d-492b62e0a8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Naloxone reverses the antihypertensive effect of clonidine. In unanesthetized, spontaneously hypertensive rats the decrease in blood pressure and heart rate produced by intravenous clonidine, 5 to 20 micrograms/kg, was inhibited or reversed by nalozone, 0.2 to 2 mg/kg. The hypotensive effect of 100 mg/kg alpha-methyldopa was also partially reversed by naloxone. Naloxone alone did not affect either blood pressure or heart rate. In brain membranes from spontaneously hypertensive rats clonidine, 10(-8) to 10(-5) M, did not influence stereoselective binding of [3H]-naloxone (8 nM), and naloxone, 10(-8) to 10(-4) M, did not influence clonidine-suppressible binding of [3H]-dihydroergocryptine (1 nM). These findings indicate that in spontaneously hypertensive rats the effects of central alpha-adrenoceptor stimulation involve activation of opiate receptors. As naloxone and clonidine do not appear to interact with the same receptor site, the observed functional antagonism suggests the release of an endogenous opiate by clonidine or alpha-methyldopa and the possible role of the opiate in the central control of sympathetic tone.',\n",
       " 'relations': 'alpha-methyldopa @CHEMICAL@ hypotensive @DISEASE@ @CID@'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_example = dataset_train[0]\n",
    "single_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4008250-56c7-45eb-a0df-275499382552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1823, 24938, 782, 7211, 7, 8, 1181, 13397, 49, 324, 7, 757, 1504, 13, 3, 3903, 29, 30095, 5, 86, 73, 152, 222, 88, 17, 1601, 6, 23496, 120, 6676, 324, 7, 757, 20063, 8, 6313, 16, 1717, 1666, 11, 842, 1080, 2546, 57, 6344, 162, 10529, 3, 3903, 29, 30095, 6, 305, 12, 460, 2179, 5096, 7, 87, 8711, 6, 47, 19921, 15, 26, 42, 7211, 26, 57, 3, 29, 138, 32, 9431, 6, 3, 18189, 12, 204, 5453, 87, 8711, 5, 37, 10950, 324, 7, 757, 1504, 13, 910, 5453, 87, 8711, 491, 6977, 18, 22758, 26, 32, 102, 9, 47, 92, 14610, 7211, 26, 57, 3, 29, 9, 24938, 782, 5, 1823, 24938, 782, 2238, 410, 59, 2603, 893, 1717, 1666, 42, 842, 1080, 5, 86, 2241, 13304, 7, 45, 23496, 120, 6676, 324, 7, 757, 20063, 3, 3903, 29, 30095, 6, 335, 599, 18, 13520, 12, 335, 599, 18, 9120, 283, 6, 410, 59, 2860, 16687, 7, 15, 3437, 757, 11293, 13, 784, 519, 566, 908, 18, 29, 9, 24938, 782, 13642, 3, 29, 329, 201, 11, 3, 29, 9, 24938, 782, 6, 335, 599, 18, 13520, 12, 335, 599, 18, 7256, 283, 6, 410, 59, 2860, 3, 3903, 29, 30095, 18, 7, 413, 4715, 2317, 11293, 13, 784, 519, 566, 908, 18, 26, 23, 10656, 32, 49, 839, 13708, 630, 4077, 3, 29, 329, 137, 506, 7469, 6360, 24, 16, 23496, 120, 6676, 324, 7, 757, 20063, 8, 1951, 13, 2069, 491, 6977, 18, 9, 26, 1536, 32, 6873, 127, 22935, 7789, 5817, 257, 13, 3, 15405, 342, 15102, 7, 5, 282, 3, 29, 9, 24938, 782, 11, 3, 3903, 29, 30095, 103, 59, 2385, 12, 6815, 28, 8, 337, 15102, 353, 6, 8, 6970, 5014, 46, 2408, 32, 14378, 6490, 8, 1576, 13, 46, 414, 5255, 1162, 3, 15405, 342, 57, 3, 3903, 29, 30095, 42, 491, 6977, 18, 22758, 26, 32, 102, 9, 11, 8, 487, 1075, 13, 8, 3, 15405, 342, 16, 8, 2069, 610, 13, 29154, 5739, 5, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_output_text = tokenizer(single_example['input'])\n",
    "print(len(tokenizer_output_text['input_ids']))\n",
    "tokenizer_output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e310c938-1e43-4f34-82bb-a795b6e7c680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgrootde/Generative-re-tests/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [491, 6977, 18, 22758, 26, 32, 102, 9, 3320, 13717, 329, 23936, 1741, 10950, 324, 7, 757, 3320, 308, 19056, 17892, 1741, 3320, 254, 4309, 1741, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    tokenizer_output_rel = tokenizer(single_example['relations'])\n",
    "tokenizer_output_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4d87b52-da4a-4c07-8698-950fa9bc3f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage that comes above max sequence length: 13.4\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = tokenizer(dataset_train['input'])\n",
    "count = 0\n",
    "for idx, input_ids in enumerate(tokenized_dataset['input_ids']):\n",
    "    if len(input_ids) > 512:\n",
    "        # print(idx, len(input_ids))\n",
    "        count+=1\n",
    "print(f'Percentage that comes above max sequence length: {count/(idx+1)*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be7e66-dcfd-4dac-a6fd-48611e60a1c4",
   "metadata": {},
   "source": [
    "## Create Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf0a4504-9b5b-4f06-be6c-f6c8f8367fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_pad_token_for_loss=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "486f3af2-710c-4b7d-b466-8c388313eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    '''\n",
    "    This function takes a dataset of input and target sequences.\n",
    "    meant to be used with the dataset.map() function\n",
    "    '''\n",
    "\n",
    "    text_column = 'input'\n",
    "    rel_column = 'relations'\n",
    "\n",
    "    # Split input and target\n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(examples[text_column])):\n",
    "        if examples[text_column][i] and examples[rel_column][i]: # remove pairs where one is None\n",
    "            inputs.append(examples[text_column][i])\n",
    "            targets.append(examples[rel_column][i])\n",
    "\n",
    "    # Tokenize the input\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=512, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Tokenize the target sequence\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets, \n",
    "            max_length=512, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "    # Replace pad tokens with -100 so they don't contribute too the loss\n",
    "    if ignore_pad_token_for_loss:\n",
    "        labels[\"input_ids\"] = [\n",
    "                    [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "                ]\n",
    "\n",
    "    # Add tokenized target text to output\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25166871-1b9f-4225-a7ef-e9c1ed1a2261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on train dataset: 100%|██████████| 500/500 [00:02<00:00, 223.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset_train.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Running tokenizer on train dataset\"\n",
    ")\n",
    "\n",
    "eval_dataset = dataset_eval.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Running tokenizer on train dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7374863-ac54-48a7-a2aa-735d3605448b",
   "metadata": {},
   "source": [
    "## Create evaluation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5415e42d-0caf-40ca-83bb-7976c088d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d5b78e2-9a33-4b92-8e23-5cb96cd1c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1fedda-d4e2-4422-b2cb-0e324d95478e",
   "metadata": {},
   "source": [
    "We need to define two functions:\n",
    "- a postprocess function that does the post processing of a prediction\n",
    "- A compute_metrics function that takes as input an prediction and output the mean score of that prediction?\n",
    "\n",
    "the compute metrics functions alwas takes a tuple of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32375a9b-d715-4656-9d85-c851a1d2e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "        preds = [pred.strip() for pred in preds]\n",
    "        labels = [label.strip() for label in labels]\n",
    "\n",
    "        # rougeLSum expects newline after each sentence\n",
    "        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "        return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d403a11-daac-436e-92b9-3c64d6f92c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "        \n",
    "    # Replace -100s used for padding as we can't decode them\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d7014a-487b-4a7a-8e68-da0082d8385b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_re",
   "language": "python",
   "name": "gen_re"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
