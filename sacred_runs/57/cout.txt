INFO - generative_re - Running command 'main'
INFO - generative_re - Started run with ID "57"
‚†ô Loading dataset from:data/cdr_seq2rel‚†π Loading dataset from:data/cdr_seq2rel‚†∏ Loading dataset from:data/cdr_seq2rel‚†º Loading dataset from:data/cdr_seq2rel[2K[38;5;2m‚úî Loaded dataset from:data/cdr_seq2rel[0m
‚†ô Initializing tokenizer‚†π Initializing tokenizer‚†∏ Initializing tokenizer‚†º Initializing tokenizer‚†¥ Initializing tokenizer‚†¶ Initializing tokenizer‚†ß Initializing tokenizer‚†á Initializing tokenizer‚†è Initializing tokenizer‚†ô Initializing tokenizer[2K[38;5;2m‚úî Initialized Tokenizer[0m
‚†ô Loading model google-t5/t5-base‚†π Loading model google-t5/t5-base‚†∏ Loading model google-t5/t5-base‚†º Loading model google-t5/t5-base‚†¥ Loading model google-t5/t5-base‚†¶ Loading model google-t5/t5-base‚†ß Loading model google-t5/t5-base‚†á Loading model google-t5/t5-base‚†è Loading model google-t5/t5-base‚†ô Loading model google-t5/t5-base[2K[38;5;2m‚úî Loaded model google-t5/t5-base[0m
‚†ô Preprocessing dataset...Parameter 'function'=<function preprocess_function at 0x7ff1bcb7c700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
WARNING - datasets.fingerprint - Parameter 'function'=<function preprocess_function at 0x7ff1bcb7c700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Running tokenizer on train dataset:   0%|                          | 0/1000 [00:00<?, ? examples/s]‚†π Preprocessing dataset...‚†∏ Preprocessing dataset...‚†º Preprocessing dataset...‚†¥ Preprocessing dataset...‚†¶ Preprocessing dataset...‚†ß Preprocessing dataset...‚†á Preprocessing dataset...‚†è Preprocessing dataset...‚†ô Preprocessing dataset...‚†π Preprocessing dataset...‚†∏ Preprocessing dataset...‚†º Preprocessing dataset...‚†¥ Preprocessing dataset...‚†¶ Preprocessing dataset...‚†ß Preprocessing dataset...‚†á Preprocessing dataset...‚†è Preprocessing dataset...‚†ô Preprocessing dataset...‚†π Preprocessing dataset...‚†∏ Preprocessing dataset...‚†º Preprocessing dataset...‚†¥ Preprocessing dataset...‚†¶ Preprocessing dataset...‚†ß Preprocessing dataset...‚†á Preprocessing dataset...‚†è Preprocessing dataset...‚†ô Preprocessing dataset...‚†π Preprocessing dataset...‚†∏ Preprocessing dataset...‚†º Preprocessing dataset...‚†¥ Preprocessing dataset...‚†¶ Preprocessing dataset...‚†ß Preprocessing dataset...‚†á Preprocessing dataset...‚†è Preprocessing dataset...‚†ô Preprocessing dataset...‚†π Preprocessing dataset...‚†∏ Preprocessing dataset...‚†º Preprocessing dataset...‚†¥ Preprocessing dataset...‚†¶ Preprocessing dataset...Running tokenizer on train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:04<00:00, 243.68 examples/s]Running tokenizer on train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:04<00:00, 242.88 examples/s]
Running tokenizer on evaluation dataset:   0%|                      | 0/500 [00:00<?, ? examples/s]‚†ß Preprocessing dataset...‚†á Preprocessing dataset...‚†è Preprocessing dataset...‚†ô Preprocessing dataset...‚†π Preprocessing dataset...‚†∏ Preprocessing dataset...‚†º Preprocessing dataset...‚†¥ Preprocessing dataset...‚†¶ Preprocessing dataset...‚†ß Preprocessing dataset...‚†á Preprocessing dataset...‚†è Preprocessing dataset...‚†ô Preprocessing dataset...‚†π Preprocessing dataset...‚†∏ Preprocessing dataset...‚†º Preprocessing dataset...‚†¥ Preprocessing dataset...Running tokenizer on evaluation dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 281.95 examples/s]‚†¶ Preprocessing dataset...Running tokenizer on evaluation dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 280.79 examples/s]
[2K[38;5;2m‚úî Preprocessed dataset![0m
/home/lgrootde/.conda/envs/llm_gen_re/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
