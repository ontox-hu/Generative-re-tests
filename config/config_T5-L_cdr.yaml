###########################
# First version of config #
###########################

########## Notes ##########
# This config is meant to run on a single A10

#### Config ####
# The name of the Hugging Face model to be used for fine-tuning
model_name: "google-t5/t5-large"

# The name of the dataset directory
dataset_vars:
    type: "csv" #filetype 
    dir: "data/cdr_seq2rel" #directory to scan for files
    split: "train" 
    column_names:
        - "input"
        - "relations"

# The directory where the fine-tuned model is saved
output_dir: "./results"

# Local rank for distributed training, -1 for non-distributed
local_rank: -1

# Batch size per device during training
per_device_train_batch_size: 2

# Batch size per device for evaluation
per_device_eval_batch_size: 2

# Number of steps for gradient accumulation to optimize GPU memory usage
gradient_accumulation_steps: 4

# Learning rate for the optimizer
learning_rate: 0.0002

# Maximum gradient norm for gradient clipping, helps in stabilizing training
max_grad_norm: 0.3

# Weight decay coefficient for regularization
weight_decay: 0.001

# Maximum sequence length for model inputs
max_seq_length: 512

# Maximun sequence length for model output
max_target_length: 152

# Number of training epochs
num_train_epochs: 1

# Wheter to apply padding to can be either: True or 'longest'/ max_length / False or 'do_not_pad'
padding: True

# Wheter to replace the pad tokens with -100 so that they are ignored when calculating the loss
ignore_pad_token_for_loss: True

# Shortens the input / target sequence to fit the max_seq_length or max_target_length
truncation: True

# Wheter to predict with the generate function. should be true for Seq2Seq models
predict_with_generate: True

# Enables gradient checkpointing to save memory at the cost of slower backward pass
gradient_checkpointing: True

# Optimizer to use, optimized for 32-bit precision
optim: "paged_adamw_32bit"

# Type of learning rate scheduler
lr_scheduler_type: "constant"

# Maximum number of training steps
max_steps: 10000

# Warmup ratio for learning rate scheduling
warmup_ratio: 0.03

# Groups sequences by length to minimize padding, improving efficiency
group_by_length: True

# Frequency of saving the model
save_steps: 100

# Whether to run evaluation on the validation set or not. Will be set to True if evaluation_strategy is different from "no"
do_eval: True

# The evaluation strategy to adopt during training. Possible values are: no, steps, epoch
evaluation_strategy: 'steps'

# (int or float, optional) — Number of update steps between two evaluations if evaluation_strategy="steps".
eval_steps: 100

###############################################
#               Memory settings               #
###############################################

# Configuration for PyTorch CUDA memory allocation to optimize GPU memory
pytorch_cuda_alloc_conf_list:
  - "heuristic"
  - "max_split_size_mb512"

# Enables the use of 8-bit quantization to reduce memory usage
use_8bit: False

# Flags for using mixed precision training
fp16: False
bf16: False

# Frequency of logging training updates
logging_steps: 10
