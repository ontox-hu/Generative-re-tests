{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30446cb9-44dc-4b1f-8afd-423f530500f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgrootde/Generative-re-tests/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    AutoTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from wasabi import msg\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ee923f6-dca8-4afa-be9a-16bb513a3fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config\n",
    "import yaml\n",
    "with open('config/config_T5-L_cdr.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13ad9288-daf4-4801-bcbe-448e71ad114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "        config['dataset_vars']['type'], \n",
    "        data_dir=config['dataset_vars']['dir'],\n",
    "        column_names=config['dataset_vars']['column_names']\n",
    "        )\n",
    "\n",
    "eval_dataset = dataset['validation'].select(range(1,501)) # remove first row that contains column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9abf031-eac1-4679-836a-1bc258b52e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather random examples from the evaluation dataset\n",
    "amount_examples_to_show = 5\n",
    "random_examples = []\n",
    "for i in range(amount_examples_to_show):\n",
    "    pick = random.randint(0, len(eval_dataset)-1)\n",
    "    random_examples.append({'Input':eval_dataset[pick]['input'],\n",
    "                            'Expected output':eval_dataset[pick]['relations']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47975b07-a490-4c39-89a3-62d5dc9b198b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgrootde/Generative-re-tests/venv/lib/python3.12/site-packages/transformers/models/t5/tokenization_t5_fast.py:171: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on google-t5/t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = config['model_name']\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "global tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, legacy=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map\n",
    ") # we specificly use T5 for Conditional generations because it has a language modeling head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a7565d-81b7-4fd3-9fe6-2f10c6a72e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Nightmares and hallucinations after long-term intake of tramadol combined with antidepressants. Tramadol is a weak opioid with effects on adrenergic and serotonergic neurotransmission that is used to treat cancer pain and chronic non malignant pain. This drug was initiated in association with paroxetine and dosulepine hydrochloride in a tetraparetic patient with chronic pain. Fifty-six days after initiation of the treatment the patient presented hallucinations that only stopped after the withdrawal of psycho-active drugs and tramadol. The case report questions the long term use of pain killers combined with psycho-active drugs in chronic non malignant pain, especially if pain is under control.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "tramadol @CHEMICAL@ hallucinations @DISEASE@ @CID@ paroxetine @CHEMICAL@ hallucinations @DISEASE@ @CID@ dosulepine hydrochloride @CHEMICAL@ hallucinations @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "tramadol and antidepressants.....s after long-term intake of tramadol combined with antidepressants.s after long-term intake of tramadol combined with antidepressants.s ands after long-term intake of... andonergic. and.. in in in. and and the the The case report The patient \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "In vivo characterization of a dual adenosine A2A/A1 receptor antagonist in animal models of Parkinson's disease. The in vivo characterization of a dual adenosine A(2A)/A(1) receptor antagonist in several animal models of Parkinson's disease is described. Discovery and scale-up syntheses of compound 1 are described in detail, highlighting optimization steps that increased the overall yield of 1 from 10.0% to 30.5%. Compound 1 is a potent A(2A)/A(1) receptor antagonist in vitro (A(2A) K(i) = 4.1 nM; A(1) K(i) = 17.0 nM) that has excellent activity, after oral administration, across a number of animal models of Parkinson's disease including mouse and rat models of haloperidol-induced catalepsy, mouse model of reserpine-induced akinesia, rat 6-hydroxydopamine (6-OHDA) lesion model of drug-induced rotation, and MPTP-treated non-human primate model.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "haloperidol @CHEMICAL@ catalepsy @DISEASE@ @CID@ reserpine @CHEMICAL@ akinesia @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "-induced akinesia, rat model of drug-induced rotation, and human primate model of drug-induced rotation.induced rotation.induced rotation. Rat model of drug-induced rotation. Mouse model of drug-induced rotation. Rat model of drug-induced rotation. Human model. Rat model. Mouse model.. vivo vivo  vivo vivo vivo vivo vivo  viv  rat \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (677 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "The antiarrhythmic effect and possible ionic mechanisms of pilocarpine on animal models. This study was designed to evaluate the effects of pilocarpine and explore the underlying ionic mechanism, using both aconitine-induced rat and ouabain-induced guinea pig arrhythmia models. Confocal microscopy was used to measure intracellular free-calcium concentrations ([Ca(2+)](i)) in isolated myocytes. The current data showed that pilocarpine significantly delayed onset of arrhythmias, decreased the time course of ventricular tachycardia and fibrillation, reduced arrhythmia score, and increased the survival time of arrhythmic rats and guinea pigs. [Ca(2+)](i) overload induced by aconitine or ouabain was reduced in isolated myocytes pretreated with pilocarpine. Moreover, M(3)-muscarinic acetylcholine receptor (mAChR) antagonist 4-DAMP (4-diphenylacetoxy-N-methylpiperidine-methiodide) partially abolished the beneficial effects of pilocarpine. These data suggest that pilocarpine produced antiarrhythmic actions on arrhythmic rat and guinea pig models induced by aconitine or ouabain via stimulating the cardiac M(3)-mAChR. The mechanism may be related to the improvement of Ca(2+) handling.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "aconitine @CHEMICAL@ arrhythmia ; arrhythmias ; arrhythmic @DISEASE@ @CID@ ouabain @CHEMICAL@ arrhythmia ; arrhythmias ; arrhythmic @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "models. Pilocarpine has been shown to have antiarrhythmic effects on arrhythmic rat and guinea pig models induced by aconitine and ouabain. Pilocarpine may have ionic effects on cardiac arrhythmias........................ \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Effects of cisapride on symptoms and postcibal small-bowel motor function in patients with irritable bowel syndrome. BACKGROUND: Irritable bowel syndrome is a common cause of abdominal pain and discomfort and may be related to disordered gastrointestinal motility. Our aim was to assess the effects of long-term treatment with a prokinetic agent, cisapride, on postprandial jejunal motility and symptoms in the irritable bowel syndrome (IBS). METHODS: Thirty-eight patients with IBS (constipation-predominant, n = 17; diarrhoea-predominant, n = 21) underwent 24-h ambulatory jejunal manometry before and after 12 week's treatment [cisapride, 5 mg three times daily (n = 19) or placebo (n = 19)]. RESULTS: In diarrhoea-predominant patients significant differences in contraction characteristics were observed between the cisapride and placebo groups. In cisapride-treated diarrhoea-predominant patients the mean contraction amplitude was higher (29.3 +/- 3.2 versus 24.9 +/- 2.6 mm Hg, cisapride versus placebo (P < 0.001); pretreatment, 25.7 +/- 6.0 mm Hg), the mean contraction duration longer (3.4 +/- 0.2 versus 3.0 +/- 0.2 sec, cisapride versus placebo (P < 0.001); pretreatment, 3.1 +/- 0.5 sec), and the mean contraction frequency lower (2.0 +/- 0.2 versus 2.5 +/- 0.4 cont./min, cisapride versus placebo (P < 0.001); pretreatment, 2.5 +/- 1.1 cont./min] than patients treated with placebo. No significant differences in jejunal motility were found in the constipation-predominant IBS group. Symptoms were assessed by using a visual analogue scale before and after treatment. Symptom scores relating to the severity of constipation were lower in cisapride-treated constipation-predominant IBS patients [score, 54 +/- 5 versus 67 +/- 14 mm, cisapride versus placebo (P < 0.05); pretreatment, 62 +/- 19 mm]. Diarrhoea-predominant IBS patients had a higher pain score after cisapride therapy [score, 55 +/- 15 versus 34 +/- 12 mm, cisapride versus placebo (P < 0.05); pretreatment, 67 +/- 19 mm]. CONCLUSION: Cisapride affects jejunal contraction characteristics and some symptoms in IBS.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "cisapride @CHEMICAL@ abdominal pain @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "compared to pretreatment. Pain scores were lower in cisapride-treated patients [score, 55 +/- 15 versus 34 +/- 12 mm, cisapride versus placebo (P  0.05); pretreatment, 62 +/- 19 mm]. RESEARCH DESIGN: The study was conducted in patients with IBS with mild to moderate symptoms. \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Reversal of neuroleptic-induced catalepsy by novel aryl-piperazine anxiolytic drugs. The novel anxiolytic drug, buspirone, reverses catalepsy induced by haloperidol. A series of aryl-piperazine analogues of buspirone and other 5-hydroxytryptaminergic agonists were tested for their ability to reverse haloperidol induced catalepsy. Those drugs with strong affinity for 5-hydroxytryptamine1a receptors were able to reverse catalepsy. Drugs with affinity for other 5-HT receptors or weak affinity were ineffective. However, inhibition of postsynaptic 5-HT receptors neither inhibited nor potentiated reversal of catalepsy and leaves open the question as to the site or mechanism for this effect.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "haloperidol @CHEMICAL@ catalepsy @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      ". The inhibition of presynaptic 5-HT receptors.. Reversal of neuroleptic-induced catalepsy by novel aryl-piperazine anxiolytic drugs. Reversal of neuroleptic-induced catalepsy by novel anxiolytic drugs....-piperazine drugs..... and,.. and.s \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model performance before training\n",
    "inputs = [i[\"Input\"] for i in random_examples]\n",
    "expected_output = [i[\"Expected output\"] for i in random_examples]\n",
    "\n",
    "for input, expected in zip(inputs, expected_output):\n",
    "    # inference\n",
    "    input_ids = tokenizer(input, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "    output = model.generate(input_ids, max_new_tokens=128)\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # print overview\n",
    "    msg.info(\"Input:\")\n",
    "    print(input)\n",
    "    msg.good(\"Expected output:\")\n",
    "    print(expected)\n",
    "    msg.info(\"Actual output:\")\n",
    "    print(decoded_output, \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c54f4949-eec2-4e83-9312-f09db0d9b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model after training\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"fine_tune_results/checkpoint-1200\",\n",
    "    device_map=device_map,\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65dfebe5-1ed1-42ab-9b27-093edbdb5877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Nightmares and hallucinations after long-term intake of tramadol combined with antidepressants. Tramadol is a weak opioid with effects on adrenergic and serotonergic neurotransmission that is used to treat cancer pain and chronic non malignant pain. This drug was initiated in association with paroxetine and dosulepine hydrochloride in a tetraparetic patient with chronic pain. Fifty-six days after initiation of the treatment the patient presented hallucinations that only stopped after the withdrawal of psycho-active drugs and tramadol. The case report questions the long term use of pain killers combined with psycho-active drugs in chronic non malignant pain, especially if pain is under control.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "tramadol @CHEMICAL@ hallucinations @DISEASE@ @CID@ paroxetine @CHEMICAL@ hallucinations @DISEASE@ @CID@ dosulepine hydrochloride @CHEMICAL@ hallucinations @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "tramadol @CHEMICAL@ nightmares @DISEASE@ @CID@ tramadol @CHEMICAL@ hallucinations @DISEASE@ @CID@ paroxetine @CHEMICAL@ nightmares @DISEASE@ @CID@ dosulepine hydrochloride @CHEMICAL@ hallucinations @DISEASE@ @CID@ \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "In vivo characterization of a dual adenosine A2A/A1 receptor antagonist in animal models of Parkinson's disease. The in vivo characterization of a dual adenosine A(2A)/A(1) receptor antagonist in several animal models of Parkinson's disease is described. Discovery and scale-up syntheses of compound 1 are described in detail, highlighting optimization steps that increased the overall yield of 1 from 10.0% to 30.5%. Compound 1 is a potent A(2A)/A(1) receptor antagonist in vitro (A(2A) K(i) = 4.1 nM; A(1) K(i) = 17.0 nM) that has excellent activity, after oral administration, across a number of animal models of Parkinson's disease including mouse and rat models of haloperidol-induced catalepsy, mouse model of reserpine-induced akinesia, rat 6-hydroxydopamine (6-OHDA) lesion model of drug-induced rotation, and MPTP-treated non-human primate model.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "haloperidol @CHEMICAL@ catalepsy @DISEASE@ @CID@ reserpine @CHEMICAL@ akinesia @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "haloperidol @CHEMICAL@ catalepsy @DISEASE@ @CID@ reserpine @CHEMICAL@ akinesia @DISEASE@ @CID@ mptp @CHEMICAL@ catalepsy @DISEASE@ @CID@ \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "The antiarrhythmic effect and possible ionic mechanisms of pilocarpine on animal models. This study was designed to evaluate the effects of pilocarpine and explore the underlying ionic mechanism, using both aconitine-induced rat and ouabain-induced guinea pig arrhythmia models. Confocal microscopy was used to measure intracellular free-calcium concentrations ([Ca(2+)](i)) in isolated myocytes. The current data showed that pilocarpine significantly delayed onset of arrhythmias, decreased the time course of ventricular tachycardia and fibrillation, reduced arrhythmia score, and increased the survival time of arrhythmic rats and guinea pigs. [Ca(2+)](i) overload induced by aconitine or ouabain was reduced in isolated myocytes pretreated with pilocarpine. Moreover, M(3)-muscarinic acetylcholine receptor (mAChR) antagonist 4-DAMP (4-diphenylacetoxy-N-methylpiperidine-methiodide) partially abolished the beneficial effects of pilocarpine. These data suggest that pilocarpine produced antiarrhythmic actions on arrhythmic rat and guinea pig models induced by aconitine or ouabain via stimulating the cardiac M(3)-mAChR. The mechanism may be related to the improvement of Ca(2+) handling.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "aconitine @CHEMICAL@ arrhythmia ; arrhythmias ; arrhythmic @DISEASE@ @CID@ ouabain @CHEMICAL@ arrhythmia ; arrhythmias ; arrhythmic @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "pilocarpine @CHEMICAL@ arrhythmias @DISEASE@ @CID@ aconitine @CHEMICAL@ ventricular tachycardia @DISEASE@ @CID@ ouabain @CHEMICAL@ arrhythmias @DISEASE@ @CID@ \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Effects of cisapride on symptoms and postcibal small-bowel motor function in patients with irritable bowel syndrome. BACKGROUND: Irritable bowel syndrome is a common cause of abdominal pain and discomfort and may be related to disordered gastrointestinal motility. Our aim was to assess the effects of long-term treatment with a prokinetic agent, cisapride, on postprandial jejunal motility and symptoms in the irritable bowel syndrome (IBS). METHODS: Thirty-eight patients with IBS (constipation-predominant, n = 17; diarrhoea-predominant, n = 21) underwent 24-h ambulatory jejunal manometry before and after 12 week's treatment [cisapride, 5 mg three times daily (n = 19) or placebo (n = 19)]. RESULTS: In diarrhoea-predominant patients significant differences in contraction characteristics were observed between the cisapride and placebo groups. In cisapride-treated diarrhoea-predominant patients the mean contraction amplitude was higher (29.3 +/- 3.2 versus 24.9 +/- 2.6 mm Hg, cisapride versus placebo (P < 0.001); pretreatment, 25.7 +/- 6.0 mm Hg), the mean contraction duration longer (3.4 +/- 0.2 versus 3.0 +/- 0.2 sec, cisapride versus placebo (P < 0.001); pretreatment, 3.1 +/- 0.5 sec), and the mean contraction frequency lower (2.0 +/- 0.2 versus 2.5 +/- 0.4 cont./min, cisapride versus placebo (P < 0.001); pretreatment, 2.5 +/- 1.1 cont./min] than patients treated with placebo. No significant differences in jejunal motility were found in the constipation-predominant IBS group. Symptoms were assessed by using a visual analogue scale before and after treatment. Symptom scores relating to the severity of constipation were lower in cisapride-treated constipation-predominant IBS patients [score, 54 +/- 5 versus 67 +/- 14 mm, cisapride versus placebo (P < 0.05); pretreatment, 62 +/- 19 mm]. Diarrhoea-predominant IBS patients had a higher pain score after cisapride therapy [score, 55 +/- 15 versus 34 +/- 12 mm, cisapride versus placebo (P < 0.05); pretreatment, 67 +/- 19 mm]. CONCLUSION: Cisapride affects jejunal contraction characteristics and some symptoms in IBS.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "cisapride @CHEMICAL@ abdominal pain @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "cisapride @CHEMICAL@ abdominal pain @DISEASE@ @CID@ cisapride @CHEMICAL@ disordered gastrointestinal motility ; gi @DISEASE@ @CID@ \n",
      "\n",
      "\n",
      "\n",
      "\u001b[38;5;4mℹ Input:\u001b[0m\n",
      "Reversal of neuroleptic-induced catalepsy by novel aryl-piperazine anxiolytic drugs. The novel anxiolytic drug, buspirone, reverses catalepsy induced by haloperidol. A series of aryl-piperazine analogues of buspirone and other 5-hydroxytryptaminergic agonists were tested for their ability to reverse haloperidol induced catalepsy. Those drugs with strong affinity for 5-hydroxytryptamine1a receptors were able to reverse catalepsy. Drugs with affinity for other 5-HT receptors or weak affinity were ineffective. However, inhibition of postsynaptic 5-HT receptors neither inhibited nor potentiated reversal of catalepsy and leaves open the question as to the site or mechanism for this effect.\n",
      "\u001b[38;5;2m✔ Expected output:\u001b[0m\n",
      "haloperidol @CHEMICAL@ catalepsy @DISEASE@ @CID@\n",
      "\u001b[38;5;4mℹ Actual output:\u001b[0m\n",
      "haloperidol @CHEMICAL@ catalepsy @DISEASE@ @CID@ \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model performance before training\n",
    "inputs = [i[\"Input\"] for i in random_examples]\n",
    "expected_output = [i[\"Expected output\"] for i in random_examples]\n",
    "\n",
    "for input, expected in zip(inputs, expected_output):\n",
    "    # inference\n",
    "    input_ids = tokenizer(input, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "    output = model.generate(input_ids, max_new_tokens=128)\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # print overview\n",
    "    msg.info(\"Input:\")\n",
    "    print(input)\n",
    "    msg.good(\"Expected output:\")\n",
    "    print(expected)\n",
    "    msg.info(\"Actual output:\")\n",
    "    print(decoded_output, \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff52e9-b631-4aa0-b192-3b052bd13b88",
   "metadata": {},
   "source": [
    "# Evaluation using scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "859098bc-e771-4b1d-8f6e-4d34e5959ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run import *\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54e042-d118-4dbb-898a-62a24235ff58",
   "metadata": {},
   "source": [
    "### Setting up trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29fd09e2-135c-4b8a-aa50-808615ea9866",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = Seq2SeqTrainingArguments(\n",
    "        output_dir=config['output_dir'],\n",
    "        per_device_train_batch_size=config['per_device_train_batch_size'],\n",
    "        gradient_accumulation_steps=config['gradient_accumulation_steps'],\n",
    "        optim=config['optim'],\n",
    "        save_steps=config['save_steps'],\n",
    "        logging_steps=config['logging_steps'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        fp16=config['fp16'],\n",
    "        bf16=config['bf16'],\n",
    "        max_grad_norm=config['max_grad_norm'],\n",
    "        max_steps=config['max_steps'],\n",
    "        warmup_ratio=config['warmup_ratio'],\n",
    "        group_by_length=config['group_by_length'],\n",
    "        lr_scheduler_type=config['lr_scheduler_type'],\n",
    "        predict_with_generate=True,\n",
    "        save_total_limit=2,\n",
    "        save_strategy='steps',\n",
    "        load_best_model_at_end=True,\n",
    "        do_eval=config['do_eval'],\n",
    "        evaluation_strategy=config['evaluation_strategy'],\n",
    "        eval_steps=config['eval_steps'],\n",
    "        remove_unused_columns=True,\n",
    "        generation_max_length=152\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4487f4f-7bb3-459a-b7f1-f741a732c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100,\n",
    "        pad_to_multiple_of=8 if config['fp16'] else None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cf65a09-d0db-435b-9557-30fb5bf6a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    '''\n",
    "    This function takes a dataset of input and target sequences.\n",
    "    meant to be used with the dataset.map() function\n",
    "    '''\n",
    "    \n",
    "    text_column = dataset_vars['column_names'][0]\n",
    "    rel_column = dataset_vars['column_names'][1]\n",
    "\n",
    "    # Split input and target\n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(examples[text_column])):\n",
    "        if examples[text_column][i] and examples[rel_column][i]: # remove pairs where one is None\n",
    "            inputs.append(examples[text_column][i])\n",
    "            targets.append(examples[rel_column][i])\n",
    "\n",
    "    # Tokenize the input\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=max_seq_length, \n",
    "        padding=padding, \n",
    "        truncation=truncation, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Tokenize the target sequence\n",
    "    labels = tokenizer(\n",
    "        text_target=targets, \n",
    "        max_length=max_seq_length, \n",
    "        padding=padding, \n",
    "        truncation=truncation,  \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Replace pad tokens with -100 so they don't contribute too the loss\n",
    "    if ignore_pad_token_for_loss:\n",
    "        labels[\"input_ids\"] = [\n",
    "                    [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "                ]\n",
    "\n",
    "    # Add tokenized target text to output\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f41fd465-68fc-4489-973b-7ba5e81b262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "        \n",
    "    # Replace -100s used for padding as we can't decode them\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, rouge_types=['rouge1', 'rouge2'], references=decoded_labels, use_stemmer=False)\n",
    "    result.update(re_metric(predictions=decoded_preds, references=decoded_labels))\n",
    "    result.update(ner_metric(predictions=decoded_preds, references=decoded_labels, re_labels=['@CID@']))\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()} # rounds all metric values to 4 numvers behind the comma and make them percentages\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens) # mean length of the generated sequences\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71100b91-2fed-437b-9375-c02ca9ed8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vars = config['dataset_vars']\n",
    "max_seq_length = config['max_seq_length']\n",
    "padding = config['padding']\n",
    "truncation = config['truncation']\n",
    "ignore_pad_token_for_loss = config['ignore_pad_token_for_loss']\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Running tokenizer on train dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0dc72c3-5be3-464b-9522-75154bbe694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metric\n",
    "global metric # Otherwise the metric object won't be accessible from within compute_metric()\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "737025b2-4b7e-4608-94cb-3e35cd1afc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        args=training_arguments,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96a06cb4-62d9-41d4-a7a6-7a629f6acdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 03:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Generative-re-tests/venv/lib/python3.12/site-packages/transformers/trainer_seq2seq.py:166\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Generative-re-tests/venv/lib/python3.12/site-packages/transformers/trainer.py:3229\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3226\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3228\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3229\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3230\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3233\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3239\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/Generative-re-tests/venv/lib/python3.12/site-packages/transformers/trainer.py:3520\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3516\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3517\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[1;32m   3518\u001b[0m         )\n\u001b[1;32m   3519\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3520\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3521\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3522\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_preds)\u001b[0m\n\u001b[1;32m     13\u001b[0m decoded_preds, decoded_labels \u001b[38;5;241m=\u001b[39m postprocess_text(decoded_preds, decoded_labels)\n\u001b[1;32m     15\u001b[0m result \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mdecoded_preds, rouge_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge2\u001b[39m\u001b[38;5;124m'\u001b[39m], references\u001b[38;5;241m=\u001b[39mdecoded_labels, use_stemmer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m result\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mre_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoded_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoded_labels\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     17\u001b[0m result\u001b[38;5;241m.\u001b[39mupdate(ner_metric(predictions\u001b[38;5;241m=\u001b[39mdecoded_preds, references\u001b[38;5;241m=\u001b[39mdecoded_labels, re_labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@CID@\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     18\u001b[0m result \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mround\u001b[39m(v \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;66;03m# rounds all metric values to 4 numvers behind the comma and make them percentages\u001b[39;00m\n",
      "File \u001b[0;32m~/Generative-re-tests/run.py:206\u001b[0m, in \u001b[0;36mre_metric\u001b[0;34m(predictions, references)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Define groups\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pred_text, ref_text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, references):\n\u001b[0;32m--> 206\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mextract_relation_triples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m@CID@\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     references \u001b[38;5;241m=\u001b[39m extract_relation_triples(ref_text, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@CID@\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predictions:\n",
      "File \u001b[0;32m~/Generative-re-tests/run.py:91\u001b[0m, in \u001b[0;36mextract_relation_triples\u001b[0;34m(text, re_labels, keep_coreforents)\u001b[0m\n\u001b[1;32m     87\u001b[0m relations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entity_text, re_label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(entity_texts, relation_labels):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# Split head and tail entities and their labels\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     head_ent, tail_ent \u001b[38;5;241m=\u001b[39m [handle_coreforents(ent, keep_coreforents) \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(.+?)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms@(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+)@\u001b[39m\u001b[38;5;124m'\u001b[39m, entity_text)]\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# print(f\"head_ent {head_ent} | tail_ent {tail_ent}\") #DEBUG\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     re_label \u001b[38;5;241m=\u001b[39m re_label\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Generative_RE",
   "language": "python",
   "name": "gen_re"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
